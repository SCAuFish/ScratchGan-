{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading BERT and experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define functions for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"Using Tensorflow version: \" + tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "BERT_DIR = \"/home/aufish/Downloads/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try with TF2 SavedModel\n",
    "# The online downloading method does not work, use pre-downloaded module\n",
    "# bert_module = hub.Module(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\")\n",
    "\n",
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer(vocab_file, do_lower_case=False):\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer(BERT_DIR + \"/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=50):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Trial run for methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I prefer Python over Java', 'I love ice cream the best']\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = convert_sentences_to_features(sentences, tokenizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 1 in mask\n",
    "bert_inputs = [input_ids_vals, input_mask_vals, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_vals)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value for mask of one word\n",
    "import copy\n",
    "\n",
    "input_mask_val_2 = copy.deepcopy(input_mask_vals)\n",
    "input_mask_val_2[0][0] = 0\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_mask_val_2, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_val_2)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create classifier model keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add a layer to define predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictor(tf.keras.Model):\n",
    "    def __init__(self, bert_layer, class_num, drop_out=0.1):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.bert = bert_layer\n",
    "        self.drop = tf.keras.layers.Dropout(rate=drop_out)\n",
    "        self.dense= tf.keras.layers.Dense(\n",
    "            class_num,\n",
    "            activation=None,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='predictions/transform/logits')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        pooled, sequential = self.bert(inputs)\n",
    "        x = self.drop(pooled)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sanity test for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model(bert_inputs)\n",
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train BERT for Masked-word Predition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Util function to randomly mask a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "def make_rand_mask(input_ids, input_mask, vocab_size, segment_id_vals=None):\n",
    "    ''' \n",
    "    input_ids: the ids of words in the sentences\n",
    "    input_mask: initial mask (1 if there is a word; 0 for padding)\n",
    "    returns\n",
    "    input_mask: replace one bit of 1 with 0, meaning that the word will be masked\n",
    "    mask_word_ids: the id of words that are masked\n",
    "    pure_ids: ids in number instead of one-hot (to generate weights per masked word)\n",
    "    segment_id_vals: mark the masked word with segment id 1\n",
    "    '''\n",
    "    batch_size = len(input_ids)\n",
    "    \n",
    "    new_input_mask = copy.deepcopy(input_mask)\n",
    "    mask_word_ids = np.zeros((batch_size, vocab_size))\n",
    "    pure_ids = []\n",
    "    segment_encodings = []\n",
    "    for i in range(batch_size):\n",
    "        total_word = sum(input_mask[i])\n",
    "        mask_word = random.randint(0, total_word-1)\n",
    "        \n",
    "        pure_ids.append(input_ids[i][mask_word])\n",
    "        assert new_input_mask[i][mask_word] == 1\n",
    "        new_input_mask[i][mask_word] = 0\n",
    "        mask_word_ids[i][input_ids[i][mask_word]] = 1.0\n",
    "        \n",
    "        # Make the masked word segment id 1\n",
    "        assert segmend_id_vals[i][mask_word] = 0\n",
    "        segment_id_vals[i][mask_word] = 1\n",
    "                \n",
    "    return new_input_mask, tf.convert_to_tensor(mask_word_ids, dtype=tf.dtypes.float32), pure_ids, segment_id_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_masks, labels, _, segment_ids_vals = make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "result = model(bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "total_loss = 0\n",
    "with tf.GradientTape() as tape:\n",
    "    result = model(bert_inputs)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels, result)\n",
    "    total_loss += loss\n",
    "grads = tape.gradient(loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(grads) == len(model.trainable_weights)\n",
    "print(tf.reduce_sum(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load data from ScratchGan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sources have suggested that so far the company sees no reason to change its tax structures , which are perfectly legal .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_FILE = \"/home/aufish/Documents/ScratchGan++/scratchgan/emnlp_data/train.json\"\n",
    "all_sentences = json.load(open(DATA_FILE, \"r\"))\n",
    "\n",
    "all_sentences = [sentence['s'] for sentence in all_sentences]\n",
    "print(all_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mitigate unbalanced weights, count different words\n",
    "from collections import defaultdict\n",
    "\n",
    "count = defaultdict(int)\n",
    "\n",
    "max_id, max_count = 0, 0\n",
    "total_count = 0\n",
    "for sentence in all_sentences:\n",
    "    ids, _, _ = convert_sentences_to_features([sentence], tokenizer)\n",
    "    id_list = ids[0]\n",
    "    for id in id_list:\n",
    "        count[id] += 1\n",
    "        total_count += 1\n",
    "        if count[id] > max_count:\n",
    "            max_id = id\n",
    "            max_count = count[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 268586\n",
      "Number of words: 13429300\n",
      "Most frequent id: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Data size: {}\".format(len(all_sentences)))\n",
    "print(\"Number of words: {}\".format(total_count))\n",
    "print(\"Most frequent id: {}\".format(max_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_predictor(model, all_sentences, tokenizer, batch_size = 1, epoch = 1):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    data_size   = len(all_sentences)\n",
    "    print(\"Data size: {}\".format(data_size))\n",
    "    for i in range(epoch):\n",
    "        print(\"Starting epoch {}\".format(i))\n",
    "        \n",
    "        # Use the first 1000 sentences for evaluation\n",
    "        start_index = 1000\n",
    "        end_index   = start_index + batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        while start_index < data_size:\n",
    "            if start_index % 1000 == 0:\n",
    "                print(\"Sentence index: {}\\r\".format(start_index))\n",
    "                \n",
    "                print(\"Total loss: {}\".format(total_loss))\n",
    "                total_loss = 0\n",
    "\n",
    "                accuracy = eval_accuracy(model, all_sentences[:1000], tokenizer)\n",
    "                print(\"Accuracy: {}\".format(accuracy))\n",
    "            end_index = min(data_size, start_index + batch_size)\n",
    "            \n",
    "            input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "                convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "            \n",
    "            input_masks, labels, masked_ids, segment_ids_vals = \\\n",
    "                make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "    \n",
    "            bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "            weights = tf.convert_to_tensor(np.array([1 / count[id] for id in masked_ids]))\n",
    "    \n",
    "            with tf.GradientTape() as tape:\n",
    "                result = model(bert_inputs)\n",
    "                loss = tf.compat.v1.losses.softmax_cross_entropy(labels, result, weights=weights)\n",
    "                total_loss += tf.reduce_sum(loss)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            \n",
    "            opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])\n",
    "            \n",
    "            start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, all_sentences, tokenizer):\n",
    "    correct_num = 0\n",
    "    start_index = 0\n",
    "    batch_size = 10\n",
    "    end_index  = start_index + batch_size\n",
    "    \n",
    "    while start_index < len(all_sentences):\n",
    "        end_index = min(len(all_sentences), start_index + batch_size)\n",
    "        input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "            convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "\n",
    "        input_masks, labels, _, segment_ids_vals = \\\n",
    "            make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "        bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "        result = model(bert_inputs)\n",
    "\n",
    "        model_choices = tf.argmax(result, axis=1)\n",
    "        labels = tf.argmax(labels, axis=1)\n",
    "\n",
    "        equal_result = tf.equal(model_choices, labels)\n",
    "        correct_num += tf.reduce_sum(tf.cast(equal_result , tf.int32))\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    return correct_num / len(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2000\n",
      "Starting epoch 0\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.0\n",
      "Sentence index: 100\n",
      "Total loss: 0.08057606220245361\n",
      "Accuracy: 0.0\n",
      "Sentence index: 200\n",
      "Total loss: 0.05446435138583183\n",
      "Accuracy: 0.0\n",
      "Sentence index: 300\n",
      "Total loss: 0.05194595828652382\n",
      "Accuracy: 0.0\n",
      "Sentence index: 400\n",
      "Total loss: 0.10195951908826828\n",
      "Accuracy: 0.0\n",
      "Sentence index: 500\n",
      "Total loss: 0.0800694078207016\n",
      "Accuracy: 0.0\n",
      "Sentence index: 600\n",
      "Total loss: 0.058692026883363724\n",
      "Accuracy: 0.0\n",
      "Sentence index: 700\n",
      "Total loss: 0.05213850736618042\n",
      "Accuracy: 0.0\n",
      "Sentence index: 800\n",
      "Total loss: 0.10485289990901947\n",
      "Accuracy: 0.0\n",
      "Sentence index: 900\n",
      "Total loss: 0.059231966733932495\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1000\n",
      "Total loss: 0.07235340774059296\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1100\n",
      "Total loss: 0.06643466651439667\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1200\n",
      "Total loss: 0.10748433321714401\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1300\n",
      "Total loss: 0.04277251660823822\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1400\n",
      "Total loss: 0.09483958780765533\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1500\n",
      "Total loss: 0.10324807465076447\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1600\n",
      "Total loss: 0.12518641352653503\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1700\n",
      "Total loss: 0.10520324110984802\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1800\n",
      "Total loss: 0.058769553899765015\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1900\n",
      "Total loss: 0.045074716210365295\n",
      "Accuracy: 0.001\n",
      "Starting epoch 1\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.001\n",
      "Sentence index: 100\n",
      "Total loss: 0.050362538546323776\n",
      "Accuracy: 0.003\n",
      "Sentence index: 200\n",
      "Total loss: 0.05926119536161423\n",
      "Accuracy: 0.004\n",
      "Sentence index: 300\n",
      "Total loss: 0.056015659123659134\n",
      "Accuracy: 0.007\n",
      "Sentence index: 400\n",
      "Total loss: 0.09096717089414597\n",
      "Accuracy: 0.008\n",
      "Sentence index: 500\n",
      "Total loss: 0.07031497359275818\n",
      "Accuracy: 0.004\n",
      "Sentence index: 600\n",
      "Total loss: 0.05731234699487686\n",
      "Accuracy: 0.007\n",
      "Sentence index: 700\n",
      "Total loss: 0.08446160703897476\n",
      "Accuracy: 0.021\n",
      "Sentence index: 800\n",
      "Total loss: 0.08334530144929886\n",
      "Accuracy: 0.013\n",
      "Sentence index: 900\n",
      "Total loss: 0.08013027161359787\n",
      "Accuracy: 0.003\n",
      "Sentence index: 1000\n",
      "Total loss: 0.07484128326177597\n",
      "Accuracy: 0.011\n",
      "Sentence index: 1100\n",
      "Total loss: 0.0890645831823349\n",
      "Accuracy: 0.004\n",
      "Sentence index: 1200\n",
      "Total loss: 0.09497266262769699\n",
      "Accuracy: 0.004\n",
      "Sentence index: 1300\n",
      "Total loss: 0.07863412797451019\n",
      "Accuracy: 0.014\n",
      "Sentence index: 1400\n",
      "Total loss: 0.05603817105293274\n",
      "Accuracy: 0.014\n",
      "Sentence index: 1500\n",
      "Total loss: 0.07679285854101181\n",
      "Accuracy: 0.02\n",
      "Sentence index: 1600\n",
      "Total loss: 0.07998022437095642\n",
      "Accuracy: 0.009\n",
      "Sentence index: 1700\n",
      "Total loss: 0.08693232387304306\n",
      "Accuracy: 0.018\n",
      "Sentence index: 1800\n",
      "Total loss: 0.07067416608333588\n",
      "Accuracy: 0.015\n",
      "Sentence index: 1900\n",
      "Total loss: 0.16899080574512482\n",
      "Accuracy: 0.035\n",
      "Starting epoch 2\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.008\n",
      "Sentence index: 100\n",
      "Total loss: 0.06455540657043457\n",
      "Accuracy: 0.009\n",
      "Sentence index: 200\n",
      "Total loss: 0.050311703234910965\n",
      "Accuracy: 0.034\n",
      "Sentence index: 300\n",
      "Total loss: 0.04573046416044235\n",
      "Accuracy: 0.037\n",
      "Sentence index: 400\n",
      "Total loss: 0.0824522078037262\n",
      "Accuracy: 0.026\n",
      "Sentence index: 500\n",
      "Total loss: 0.1189282089471817\n",
      "Accuracy: 0.031\n",
      "Sentence index: 600\n",
      "Total loss: 0.060238316655159\n",
      "Accuracy: 0.014\n",
      "Sentence index: 700\n",
      "Total loss: 0.09010884165763855\n",
      "Accuracy: 0.015\n",
      "Sentence index: 800\n",
      "Total loss: 0.05916008725762367\n",
      "Accuracy: 0.014\n",
      "Sentence index: 900\n",
      "Total loss: 0.08005264401435852\n",
      "Accuracy: 0.016\n",
      "Sentence index: 1000\n",
      "Total loss: 0.01896648481488228\n",
      "Accuracy: 0.011\n",
      "Sentence index: 1100\n",
      "Total loss: 0.06873689591884613\n",
      "Accuracy: 0.015\n",
      "Sentence index: 1200\n",
      "Total loss: 0.07441656291484833\n",
      "Accuracy: 0.033\n",
      "Sentence index: 1300\n",
      "Total loss: 0.07740682363510132\n",
      "Accuracy: 0.025\n",
      "Sentence index: 1400\n",
      "Total loss: 0.07307910174131393\n",
      "Accuracy: 0.019\n",
      "Sentence index: 1500\n",
      "Total loss: 0.0706503614783287\n",
      "Accuracy: 0.026\n",
      "Sentence index: 1600\n",
      "Total loss: 0.07777533680200577\n",
      "Accuracy: 0.031\n",
      "Sentence index: 1700\n",
      "Total loss: 0.06260479986667633\n",
      "Accuracy: 0.033\n",
      "Sentence index: 1800\n",
      "Total loss: 0.0570494718849659\n",
      "Accuracy: 0.025\n",
      "Sentence index: 1900\n",
      "Total loss: 0.08163995295763016\n",
      "Accuracy: 0.045\n",
      "Starting epoch 3\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.033\n",
      "Sentence index: 100\n",
      "Total loss: 0.08004197478294373\n",
      "Accuracy: 0.04\n",
      "Sentence index: 200\n",
      "Total loss: 0.06740965694189072\n",
      "Accuracy: 0.039\n",
      "Sentence index: 300\n",
      "Total loss: 0.055776406079530716\n",
      "Accuracy: 0.033\n",
      "Sentence index: 400\n",
      "Total loss: 0.07375124841928482\n",
      "Accuracy: 0.043\n",
      "Sentence index: 500\n",
      "Total loss: 0.06095112860202789\n",
      "Accuracy: 0.034\n",
      "Sentence index: 600\n",
      "Total loss: 0.07933025062084198\n",
      "Accuracy: 0.037\n",
      "Sentence index: 700\n",
      "Total loss: 0.08512023091316223\n",
      "Accuracy: 0.032\n",
      "Sentence index: 800\n",
      "Total loss: 0.039242129772901535\n",
      "Accuracy: 0.034\n",
      "Sentence index: 900\n",
      "Total loss: 0.06347449123859406\n",
      "Accuracy: 0.034\n",
      "Sentence index: 1000\n",
      "Total loss: 0.06363418698310852\n",
      "Accuracy: 0.032\n",
      "Sentence index: 1100\n",
      "Total loss: 0.05495469644665718\n",
      "Accuracy: 0.027\n",
      "Sentence index: 1200\n",
      "Total loss: 0.04004588723182678\n",
      "Accuracy: 0.023\n",
      "Sentence index: 1300\n",
      "Total loss: 0.06291281431913376\n",
      "Accuracy: 0.03\n",
      "Sentence index: 1400\n",
      "Total loss: 0.05717645213007927\n",
      "Accuracy: 0.027\n",
      "Sentence index: 1500\n",
      "Total loss: 0.08955075591802597\n",
      "Accuracy: 0.026\n",
      "Sentence index: 1600\n",
      "Total loss: 0.10734207183122635\n",
      "Accuracy: 0.019\n",
      "Sentence index: 1700\n",
      "Total loss: 0.06418950110673904\n",
      "Accuracy: 0.029\n",
      "Sentence index: 1800\n",
      "Total loss: 0.06787809729576111\n",
      "Accuracy: 0.028\n",
      "Sentence index: 1900\n",
      "Total loss: 0.05534064769744873\n",
      "Accuracy: 0.025\n",
      "Starting epoch 4\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.035\n",
      "Sentence index: 100\n",
      "Total loss: 0.07205704599618912\n",
      "Accuracy: 0.051\n",
      "Sentence index: 200\n",
      "Total loss: 0.08258934319019318\n",
      "Accuracy: 0.059\n",
      "Sentence index: 300\n",
      "Total loss: 0.07811886072158813\n",
      "Accuracy: 0.054\n",
      "Sentence index: 400\n",
      "Total loss: 0.07676346600055695\n",
      "Accuracy: 0.048\n",
      "Sentence index: 500\n",
      "Total loss: 0.09632434695959091\n",
      "Accuracy: 0.045\n",
      "Sentence index: 600\n",
      "Total loss: 0.0825754702091217\n",
      "Accuracy: 0.059\n",
      "Sentence index: 700\n",
      "Total loss: 0.10176849365234375\n",
      "Accuracy: 0.048\n",
      "Sentence index: 800\n",
      "Total loss: 0.08233422040939331\n",
      "Accuracy: 0.055\n",
      "Sentence index: 900\n",
      "Total loss: 0.05560235679149628\n",
      "Accuracy: 0.063\n",
      "Sentence index: 1000\n",
      "Total loss: 0.06889794766902924\n",
      "Accuracy: 0.059\n",
      "Sentence index: 1100\n",
      "Total loss: 0.043745677918195724\n",
      "Accuracy: 0.07\n",
      "Sentence index: 1200\n",
      "Total loss: 0.10837576538324356\n",
      "Accuracy: 0.065\n",
      "Sentence index: 1300\n",
      "Total loss: 0.09841751307249069\n",
      "Accuracy: 0.067\n",
      "Sentence index: 1400\n",
      "Total loss: 0.08370983600616455\n",
      "Accuracy: 0.063\n",
      "Sentence index: 1500\n",
      "Total loss: 0.0668744295835495\n",
      "Accuracy: 0.075\n",
      "Sentence index: 1600\n",
      "Total loss: 0.0786823108792305\n",
      "Accuracy: 0.094\n",
      "Sentence index: 1700\n",
      "Total loss: 0.07130911946296692\n",
      "Accuracy: 0.079\n",
      "Sentence index: 1800\n",
      "Total loss: 0.07377322763204575\n",
      "Accuracy: 0.089\n",
      "Sentence index: 1900\n",
      "Total loss: 0.08295238763093948\n",
      "Accuracy: 0.061\n",
      "Starting epoch 5\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.079\n",
      "Sentence index: 100\n",
      "Total loss: 0.05104343593120575\n",
      "Accuracy: 0.084\n",
      "Sentence index: 200\n",
      "Total loss: 0.06164560094475746\n",
      "Accuracy: 0.061\n",
      "Sentence index: 300\n",
      "Total loss: 0.07952463626861572\n",
      "Accuracy: 0.069\n",
      "Sentence index: 400\n",
      "Total loss: 0.09389623999595642\n",
      "Accuracy: 0.061\n",
      "Sentence index: 500\n",
      "Total loss: 0.06952265650033951\n",
      "Accuracy: 0.04\n",
      "Sentence index: 600\n",
      "Total loss: 0.06992583721876144\n",
      "Accuracy: 0.054\n",
      "Sentence index: 700\n",
      "Total loss: 0.0852583795785904\n",
      "Accuracy: 0.06\n",
      "Sentence index: 800\n",
      "Total loss: 0.0846152976155281\n",
      "Accuracy: 0.039\n",
      "Sentence index: 900\n",
      "Total loss: 0.07695561647415161\n",
      "Accuracy: 0.036\n",
      "Sentence index: 1000\n",
      "Total loss: 0.061078667640686035\n",
      "Accuracy: 0.044\n",
      "Sentence index: 1100\n",
      "Total loss: 0.06793917715549469\n",
      "Accuracy: 0.058\n",
      "Sentence index: 1200\n",
      "Total loss: 0.06816749274730682\n",
      "Accuracy: 0.058\n",
      "Sentence index: 1300\n",
      "Total loss: 0.09746164083480835\n",
      "Accuracy: 0.061\n",
      "Sentence index: 1400\n",
      "Total loss: 0.07391460984945297\n",
      "Accuracy: 0.066\n",
      "Sentence index: 1500\n",
      "Total loss: 0.08924224972724915\n",
      "Accuracy: 0.078\n",
      "Sentence index: 1600\n",
      "Total loss: 0.05987882986664772\n",
      "Accuracy: 0.06\n",
      "Sentence index: 1700\n",
      "Total loss: 0.0631704032421112\n",
      "Accuracy: 0.077\n",
      "Sentence index: 1800\n",
      "Total loss: 0.03701242804527283\n",
      "Accuracy: 0.079\n",
      "Sentence index: 1900\n",
      "Total loss: 0.054305803030729294\n",
      "Accuracy: 0.068\n",
      "Starting epoch 6\n",
      "Sentence index: 0\n",
      "Total loss: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.075\n",
      "Sentence index: 100\n",
      "Total loss: 0.05028447508811951\n",
      "Accuracy: 0.086\n",
      "Sentence index: 200\n",
      "Total loss: 0.06859728693962097\n",
      "Accuracy: 0.075\n",
      "Sentence index: 300\n",
      "Total loss: 0.04195752739906311\n",
      "Accuracy: 0.068\n",
      "Sentence index: 400\n",
      "Total loss: 0.05924701318144798\n",
      "Accuracy: 0.062\n",
      "Sentence index: 500\n",
      "Total loss: 0.07010948657989502\n",
      "Accuracy: 0.075\n",
      "Sentence index: 600\n",
      "Total loss: 0.07927380502223969\n",
      "Accuracy: 0.052\n",
      "Sentence index: 700\n",
      "Total loss: 0.0774344876408577\n",
      "Accuracy: 0.055\n",
      "Sentence index: 800\n",
      "Total loss: 0.051811426877975464\n",
      "Accuracy: 0.06\n",
      "Sentence index: 900\n",
      "Total loss: 0.09306452423334122\n",
      "Accuracy: 0.079\n",
      "Sentence index: 1000\n",
      "Total loss: 0.071143239736557\n",
      "Accuracy: 0.08\n",
      "Sentence index: 1100\n",
      "Total loss: 0.10416867583990097\n",
      "Accuracy: 0.052\n",
      "Sentence index: 1200\n",
      "Total loss: 0.101884625852108\n",
      "Accuracy: 0.077\n",
      "Sentence index: 1300\n",
      "Total loss: 0.043635230511426926\n",
      "Accuracy: 0.062\n",
      "Sentence index: 1400\n",
      "Total loss: 0.11085750162601471\n",
      "Accuracy: 0.068\n",
      "Sentence index: 1500\n",
      "Total loss: 0.06904929876327515\n",
      "Accuracy: 0.071\n",
      "Sentence index: 1600\n",
      "Total loss: 0.07093848288059235\n",
      "Accuracy: 0.065\n",
      "Sentence index: 1700\n",
      "Total loss: 0.07965400815010071\n",
      "Accuracy: 0.064\n",
      "Sentence index: 1800\n",
      "Total loss: 0.10563646256923676\n",
      "Accuracy: 0.077\n",
      "Sentence index: 1900\n",
      "Total loss: 0.08758459985256195\n",
      "Accuracy: 0.079\n",
      "Starting epoch 7\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.062\n",
      "Sentence index: 100\n",
      "Total loss: 0.0861317589879036\n",
      "Accuracy: 0.07\n",
      "Sentence index: 200\n",
      "Total loss: 0.050924576818943024\n",
      "Accuracy: 0.073\n",
      "Sentence index: 300\n",
      "Total loss: 0.03757423162460327\n",
      "Accuracy: 0.066\n",
      "Sentence index: 400\n",
      "Total loss: 0.06806669384241104\n",
      "Accuracy: 0.083\n",
      "Sentence index: 500\n",
      "Total loss: 0.05536159873008728\n",
      "Accuracy: 0.076\n",
      "Sentence index: 600\n",
      "Total loss: 0.06069539859890938\n",
      "Accuracy: 0.083\n",
      "Sentence index: 700\n",
      "Total loss: 0.07603717595338821\n",
      "Accuracy: 0.078\n",
      "Sentence index: 800\n",
      "Total loss: 0.07757654041051865\n",
      "Accuracy: 0.069\n",
      "Sentence index: 900\n",
      "Total loss: 0.07322430610656738\n",
      "Accuracy: 0.079\n",
      "Sentence index: 1000\n",
      "Total loss: 0.08477489650249481\n",
      "Accuracy: 0.057\n",
      "Sentence index: 1100\n",
      "Total loss: 0.07076369225978851\n",
      "Accuracy: 0.077\n",
      "Sentence index: 1200\n",
      "Total loss: 0.09576013684272766\n",
      "Accuracy: 0.111\n",
      "Sentence index: 1300\n",
      "Total loss: 0.08719411492347717\n",
      "Accuracy: 0.073\n",
      "Sentence index: 1400\n",
      "Total loss: 0.07886752486228943\n",
      "Accuracy: 0.073\n",
      "Sentence index: 1500\n",
      "Total loss: 0.06158413738012314\n",
      "Accuracy: 0.089\n",
      "Sentence index: 1600\n",
      "Total loss: 0.08712927997112274\n",
      "Accuracy: 0.089\n",
      "Sentence index: 1700\n",
      "Total loss: 0.09301207214593887\n",
      "Accuracy: 0.074\n",
      "Sentence index: 1800\n",
      "Total loss: 0.048153650015592575\n",
      "Accuracy: 0.074\n",
      "Sentence index: 1900\n",
      "Total loss: 0.07493201643228531\n",
      "Accuracy: 0.063\n",
      "Starting epoch 8\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.072\n",
      "Sentence index: 100\n",
      "Total loss: 0.1018618494272232\n",
      "Accuracy: 0.07\n",
      "Sentence index: 200\n",
      "Total loss: 0.05327270179986954\n",
      "Accuracy: 0.089\n",
      "Sentence index: 300\n",
      "Total loss: 0.04309530556201935\n",
      "Accuracy: 0.076\n",
      "Sentence index: 400\n",
      "Total loss: 0.0635281503200531\n",
      "Accuracy: 0.058\n",
      "Sentence index: 500\n",
      "Total loss: 0.10249973833560944\n",
      "Accuracy: 0.077\n",
      "Sentence index: 600\n",
      "Total loss: 0.06890007853507996\n",
      "Accuracy: 0.065\n",
      "Sentence index: 700\n",
      "Total loss: 0.06788612902164459\n",
      "Accuracy: 0.061\n",
      "Sentence index: 800\n",
      "Total loss: 0.07014000415802002\n",
      "Accuracy: 0.07\n",
      "Sentence index: 900\n",
      "Total loss: 0.06307527422904968\n",
      "Accuracy: 0.063\n",
      "Sentence index: 1000\n",
      "Total loss: 0.06286793947219849\n",
      "Accuracy: 0.059\n",
      "Sentence index: 1100\n",
      "Total loss: 0.07555077224969864\n",
      "Accuracy: 0.075\n",
      "Sentence index: 1200\n",
      "Total loss: 0.07678313553333282\n",
      "Accuracy: 0.07\n",
      "Sentence index: 1300\n",
      "Total loss: 0.052142929285764694\n",
      "Accuracy: 0.062\n",
      "Sentence index: 1400\n",
      "Total loss: 0.07110891491174698\n",
      "Accuracy: 0.058\n",
      "Sentence index: 1500\n",
      "Total loss: 0.07765195518732071\n",
      "Accuracy: 0.069\n",
      "Sentence index: 1600\n",
      "Total loss: 0.08430011570453644\n",
      "Accuracy: 0.054\n",
      "Sentence index: 1700\n",
      "Total loss: 0.03928571939468384\n",
      "Accuracy: 0.065\n",
      "Sentence index: 1800\n",
      "Total loss: 0.06285163760185242\n",
      "Accuracy: 0.068\n",
      "Sentence index: 1900\n",
      "Total loss: 0.08268057554960251\n",
      "Accuracy: 0.074\n",
      "Starting epoch 9\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.086\n",
      "Sentence index: 100\n",
      "Total loss: 0.0710211843252182\n",
      "Accuracy: 0.066\n",
      "Sentence index: 200\n",
      "Total loss: 0.05459906905889511\n",
      "Accuracy: 0.06\n",
      "Sentence index: 300\n",
      "Total loss: 0.08854836225509644\n",
      "Accuracy: 0.073\n",
      "Sentence index: 400\n",
      "Total loss: 0.08377626538276672\n",
      "Accuracy: 0.072\n",
      "Sentence index: 500\n",
      "Total loss: 0.07729454338550568\n",
      "Accuracy: 0.07\n",
      "Sentence index: 600\n",
      "Total loss: 0.07797285914421082\n",
      "Accuracy: 0.064\n",
      "Sentence index: 700\n",
      "Total loss: 0.062134671956300735\n",
      "Accuracy: 0.071\n",
      "Sentence index: 800\n",
      "Total loss: 0.05342646315693855\n",
      "Accuracy: 0.056\n",
      "Sentence index: 900\n",
      "Total loss: 0.03753162547945976\n",
      "Accuracy: 0.063\n",
      "Sentence index: 1000\n",
      "Total loss: 0.08229584246873856\n",
      "Accuracy: 0.075\n",
      "Sentence index: 1100\n",
      "Total loss: 0.12139704823493958\n",
      "Accuracy: 0.071\n",
      "Sentence index: 1200\n",
      "Total loss: 0.08131852000951767\n",
      "Accuracy: 0.071\n",
      "Sentence index: 1300\n",
      "Total loss: 0.09845343232154846\n",
      "Accuracy: 0.06\n",
      "Sentence index: 1400\n",
      "Total loss: 0.060322824865579605\n",
      "Accuracy: 0.079\n",
      "Sentence index: 1500\n",
      "Total loss: 0.04696740210056305\n",
      "Accuracy: 0.065\n",
      "Sentence index: 1600\n",
      "Total loss: 0.054236769676208496\n",
      "Accuracy: 0.075\n",
      "Sentence index: 1700\n",
      "Total loss: 0.0964326411485672\n",
      "Accuracy: 0.087\n",
      "Sentence index: 1800\n",
      "Total loss: 0.05797325447201729\n",
      "Accuracy: 0.057\n",
      "Sentence index: 1900\n",
      "Total loss: 0.06856425106525421\n",
      "Accuracy: 0.076\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "\n",
    "train_word_predictor(model, all_sentences, tokenizer, batch_size=10, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, \"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model3 = tf.saved_model.load(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Qualitatively check predicting result of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [\"Governments are urged to take steps to protect the rights of women and children.\"]\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "    convert_sentences_to_features(all_sentences, tokenizer)\n",
    "\n",
    "input_masks, labels, _, segment_ids_vals = \\\n",
    "    make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "result = model(bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]\n",
      "tf.Tensor([1321], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(input_masks)\n",
    "print(tf.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word: ['take']\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing word: {}\".format(tokenizer.convert_ids_to_tokens([1321])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=\n",
       "array([20495,  4986, 25232,   500, 22621, 21363, 22785, 15454, 22321,\n",
       "       25657], dtype=int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(result, axis=1, direction=\"DESCENDING\")[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7388558, shape=(), dtype=float32)\n",
      "['Buffy']\n",
      "tf.Tensor(0.71015686, shape=(), dtype=float32)\n",
      "['roughly']\n",
      "tf.Tensor(0.7035765, shape=(), dtype=float32)\n",
      "['##rogate']\n",
      "tf.Tensor(0.6961367, shape=(), dtype=float32)\n",
      "['Ñ‰']\n",
      "tf.Tensor(0.6836092, shape=(), dtype=float32)\n",
      "['wrestled']\n",
      "tf.Tensor(0.6822932, shape=(), dtype=float32)\n",
      "['lending']\n",
      "tf.Tensor(0.67473334, shape=(), dtype=float32)\n",
      "['##gets']\n",
      "tf.Tensor(0.67315876, shape=(), dtype=float32)\n",
      "['##gnant']\n",
      "tf.Tensor(0.66435343, shape=(), dtype=float32)\n",
      "['salute']\n",
      "tf.Tensor(0.6582221, shape=(), dtype=float32)\n",
      "['concerto']\n"
     ]
    }
   ],
   "source": [
    "candidates = [20495,  4986, 25232,   500, 22621, 21363, 22785, 15454, 22321,\n",
    "       25657]\n",
    "for candidate in candidates:\n",
    "    print(result[0, candidate])\n",
    "    print(tokenizer.convert_ids_to_tokens([candidate]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
