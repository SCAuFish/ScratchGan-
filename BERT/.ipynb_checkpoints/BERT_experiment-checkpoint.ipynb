{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading BERT and experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define functions for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"Using Tensorflow version: \" + tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "BERT_DIR = \"/home/aufish/Downloads/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try with TF2 SavedModel\n",
    "# The online downloading method does not work, use pre-downloaded module\n",
    "# bert_module = hub.Module(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\")\n",
    "\n",
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer(vocab_file, do_lower_case=False):\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer(BERT_DIR + \"/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are actually not used\n",
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=50):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Trial run for methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I prefer Python over Java', 'I love ice cream the best']\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = convert_sentences_to_features(sentences, tokenizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 146, 9353, 23334, 1166, 9155, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 146, 1567, 2854, 7081, 1103, 1436, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "tf.Tensor(\n",
      "[[-0.8131681   0.5370047   0.99996513 ...  0.999987   -0.5378665\n",
      "   0.9919909 ]\n",
      " [-0.60621274  0.49540225  0.99991035 ...  0.9999771  -0.91028047\n",
      "   0.9931733 ]], shape=(2, 768), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 0.8713086   0.00939399  0.02123968 ... -0.2102821   0.45887846\n",
      "    0.23217922]\n",
      "  [ 0.5732106  -0.13002536  0.33161387 ...  0.05539769 -0.10422937\n",
      "    0.3469553 ]\n",
      "  [ 0.42146546 -0.2859141  -0.23261456 ...  0.25038135 -0.60215116\n",
      "    0.20282936]\n",
      "  ...\n",
      "  [ 0.37351245  0.26639333  0.43967193 ... -0.1845916   0.40790993\n",
      "    0.4635392 ]\n",
      "  [ 0.38079703  0.18587998  0.27819872 ... -0.21876818  0.43730605\n",
      "    0.39033228]\n",
      "  [ 0.28391755  0.11993123  0.34593338 ...  0.01512105  0.33790368\n",
      "    0.16361022]]\n",
      "\n",
      " [[ 0.49074587  0.160682    0.03785331 ... -0.23446186  0.2617393\n",
      "    0.0903241 ]\n",
      "  [ 0.13407     0.2104078   0.25069734 ...  0.23977542 -0.05616882\n",
      "    0.32233608]\n",
      "  [-0.05624516  0.01800643 -0.16432334 ...  0.65063506 -1.0328202\n",
      "    0.10341886]\n",
      "  ...\n",
      "  [ 0.05746117  0.23868068  0.09878956 ...  0.37608162 -0.19921656\n",
      "   -0.34361216]\n",
      "  [-0.01977242  0.24953367  0.2952392  ...  0.15219964 -0.02327026\n",
      "   -0.05462281]\n",
      "  [-0.13560185  0.38314173  0.36137334 ...  0.15484728 -0.12740727\n",
      "   -0.18449263]]], shape=(2, 20, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# all 1 in mask\n",
    "bert_inputs = [input_ids_vals, input_mask_vals, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_vals)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value for mask of one word\n",
    "import copy\n",
    "\n",
    "input_mask_val_2 = copy.deepcopy(input_mask_vals)\n",
    "input_mask_val_2[0][0] = 0\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_mask_val_2, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_val_2)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create classifier model keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add a layer to define predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictor(tf.keras.Model):\n",
    "    def __init__(self, bert_layer, class_num, drop_out=0.1):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.bert = bert_layer\n",
    "        self.drop1 = tf.keras.layers.Dropout(rate=drop_out)\n",
    "        self.dense1= tf.keras.layers.Dense(\n",
    "            768,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='predictions/transform/hidden')\n",
    "        \n",
    "        self.drop2 = tf.keras.layers.Dropout(rate=drop_out)\n",
    "        self.dense2 = tf.keras.layers.Dense(\n",
    "            class_num,\n",
    "            activation = None, \n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='predictions/transform/final')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        bert_layer_input, masked_word_ids = inputs[:-1], inputs[-1]\n",
    "        pooled, sequential = self.bert(inputs)\n",
    "        # use sequential instead of pool\n",
    "        x = self.drop1(pooled)\n",
    "        x = self.dense1(x)\n",
    "        x = self.drop2(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sanity test for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model(bert_inputs)\n",
    "for weight in model.trainable_weights:\n",
    "    print(weight.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train BERT for Masked-word Predition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Util function to randomly mask a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "def make_rand_mask(input_ids, input_mask, vocab_size, segment_id_vals=None):\n",
    "    ''' \n",
    "    input_ids: the ids of words in the sentences\n",
    "    input_mask: initial mask (1 if there is a word; 0 for padding)\n",
    "    returns\n",
    "    input_mask: replace one bit of 1 with 0, meaning that the word will be masked\n",
    "    mask_word_ids: the id of words that are masked\n",
    "    pure_ids: ids in number instead of one-hot (to generate weights per masked word)\n",
    "    segment_id_vals: mark the masked word with segment id 1\n",
    "    '''\n",
    "    batch_size = len(input_ids)\n",
    "    \n",
    "    new_input_mask = copy.deepcopy(input_mask)\n",
    "    mask_word_ids = np.zeros((batch_size, vocab_size))\n",
    "    pure_ids = []\n",
    "    segment_encodings = []\n",
    "    for i in range(batch_size):\n",
    "        total_word = sum(input_mask[i])\n",
    "        mask_word = random.randint(0, total_word-1)\n",
    "        \n",
    "        pure_ids.append(input_ids[i][mask_word])\n",
    "        assert new_input_mask[i][mask_word] == 1\n",
    "        new_input_mask[i][mask_word] = 0\n",
    "        mask_word_ids[i][input_ids[i][mask_word]] = 1.0\n",
    "        \n",
    "        # Make the masked word segment id 1\n",
    "        # assert segment_id_vals[i][mask_word] == 0\n",
    "        # segment_id_vals[i][mask_word] = 1\n",
    "                \n",
    "    return new_input_mask, tf.convert_to_tensor(mask_word_ids, dtype=tf.dtypes.float32), pure_ids, segment_id_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_masks, labels, _, segment_ids_vals = make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "result = model(bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "total_loss = 0\n",
    "with tf.GradientTape() as tape:\n",
    "    result = model(bert_inputs)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels, result)\n",
    "    total_loss += loss\n",
    "grads = tape.gradient(loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(grads) == len(model.trainable_weights)\n",
    "print(tf.reduce_sum(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load data from ScratchGan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_FILE = \"/home/aufish/Documents/ScratchGan++/scratchgan/emnlp_data/train.json\"\n",
    "all_sentences = json.load(open(DATA_FILE, \"r\"))\n",
    "\n",
    "all_sentences = [sentence['s'] for sentence in all_sentences]\n",
    "print(all_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mitigate unbalanced weights, count different words\n",
    "from collections import defaultdict\n",
    "\n",
    "count = defaultdict(int)\n",
    "\n",
    "max_id, max_count = 0, 0\n",
    "total_count = 0\n",
    "for sentence in all_sentences:\n",
    "    ids, _, _ = convert_sentences_to_features([sentence], tokenizer)\n",
    "    id_list = ids[0]\n",
    "    for id in id_list:\n",
    "        count[id] += 1\n",
    "        total_count += 1\n",
    "        if count[id] > max_count:\n",
    "            max_id = id\n",
    "            max_count = count[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data size: {}\".format(len(all_sentences)))\n",
    "print(\"Number of words: {}\".format(total_count))\n",
    "print(\"Most frequent id: {}\".format(max_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_predictor(model, all_sentences, tokenizer, batch_size = 1, epoch = 1):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    data_size   = len(all_sentences)\n",
    "    print(\"Data size: {}\".format(data_size))\n",
    "    for i in range(epoch):\n",
    "        print(\"Starting epoch {}\".format(i))\n",
    "        \n",
    "        # Use the first 1000 sentences for evaluation\n",
    "        start_index = 0\n",
    "        end_index   = start_index + batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        while start_index < data_size:\n",
    "            if start_index % 100 == 0:\n",
    "                print(\"Sentence index: {}\\r\".format(start_index))\n",
    "                \n",
    "                print(\"Total loss: {}\".format(total_loss))\n",
    "                total_loss = 0\n",
    "\n",
    "                accuracy = eval_accuracy(model, all_sentences[:1000], tokenizer)\n",
    "                print(\"Accuracy: {}\".format(accuracy))\n",
    "            end_index = min(data_size, start_index + batch_size)\n",
    "            \n",
    "            input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "                convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "            \n",
    "            input_masks, labels, masked_ids, segment_ids_vals = \\\n",
    "                make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "    \n",
    "            bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "            weights = tf.convert_to_tensor(np.array([1 / count[id] for id in masked_ids]))\n",
    "    \n",
    "            with tf.GradientTape() as tape:\n",
    "                result = model(bert_inputs)\n",
    "                loss = tf.compat.v1.losses.softmax_cross_entropy(labels, result, weights=weights)\n",
    "                total_loss += tf.reduce_sum(loss)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            \n",
    "            opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])\n",
    "            \n",
    "            start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, all_sentences, tokenizer):\n",
    "    correct_num = 0\n",
    "    start_index = 0\n",
    "    batch_size = 10\n",
    "    end_index  = start_index + batch_size\n",
    "    \n",
    "    while start_index < len(all_sentences):\n",
    "        end_index = min(len(all_sentences), start_index + batch_size)\n",
    "        input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "            convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "\n",
    "        input_masks, labels, _, segment_ids_vals = \\\n",
    "            make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "        bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "        result = model(bert_inputs)\n",
    "\n",
    "        model_choices = tf.argmax(result, axis=1)\n",
    "        labels = tf.argmax(labels, axis=1)\n",
    "\n",
    "        equal_result = tf.equal(model_choices, labels)\n",
    "        correct_num += tf.reduce_sum(tf.cast(equal_result , tf.int32))\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    return correct_num / len(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "\n",
    "train_word_predictor(model, all_sentences[:1000], tokenizer, batch_size=10, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./trained_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model.load_weights(\"./trained_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Qualitatively check predicting result of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [\"My sources have suggested that so far the company sees no reason to change its tax structures , which are perfectly legal .\"]\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "    convert_sentences_to_features(all_sentences, tokenizer)\n",
    "\n",
    "input_masks, labels, _, segment_ids_vals = \\\n",
    "    make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "result = model(bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_masks)\n",
    "print(tf.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing word: {}\".format(tokenizer.convert_ids_to_tokens([3641])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argsort(result, axis=1, direction=\"DESCENDING\")[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [ 101, 1103, 1106,  119, 4036, 2674, 1146,  170, 1142,  102]\n",
    "for candidate in candidates:\n",
    "    print(result[0, candidate])\n",
    "    print(tokenizer.convert_ids_to_tokens([candidate]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
