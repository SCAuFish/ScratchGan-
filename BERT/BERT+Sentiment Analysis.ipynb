{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"Using Tensorflow version: \" + tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "BERT_DIR = \"/home/aufish/Downloads/bert\"\n",
    "\n",
    "# try with TF2 SavedModel\n",
    "# The online downloading method does not work, use pre-downloaded module\n",
    "# bert_module = hub.Module(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\")\n",
    "\n",
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ 101,  102, 1106, 1103,  119,  117,  112,  170, 1122,  146]# tokenizer\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer(vocab_file, do_lower_case=False):\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer(BERT_DIR + \"/assets/vocab.txt\")\n",
    "\n",
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len=50):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=50):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids\n",
    "\n",
    "import random, copy\n",
    "import numpy as np\n",
    "def make_rand_mask(input_ids, input_mask, vocab_size, segment_id_vals=None):\n",
    "    ''' \n",
    "    input_ids: the ids of words in the sentences\n",
    "    input_mask: initial mask (1 if there is a word; 0 for padding)\n",
    "    returns\n",
    "    input_mask: replace one bit of 1 with 0, meaning that the word will be masked\n",
    "    mask_word_ids: the id of words that are masked\n",
    "    pure_ids: ids in number instead of one-hot (to generate weights per masked word)\n",
    "    segment_id_vals: mark the masked word with segment id 1\n",
    "    '''\n",
    "    batch_size = len(input_ids)\n",
    "    \n",
    "    new_input_mask = copy.deepcopy(input_mask)\n",
    "    mask_word_ids = np.zeros((batch_size, vocab_size))\n",
    "    pure_ids = []\n",
    "    segment_encodings = []\n",
    "    for i in range(batch_size):\n",
    "        total_word = sum(input_mask[i])\n",
    "        mask_word = random.randint(0, total_word-1)\n",
    "        \n",
    "        pure_ids.append(input_ids[i][mask_word])\n",
    "        assert new_input_mask[i][mask_word] == 1\n",
    "        new_input_mask[i][mask_word] = 0\n",
    "        mask_word_ids[i][input_ids[i][mask_word]] = 1.0\n",
    "        \n",
    "        # Make the masked word segment id 1\n",
    "        assert segment_id_vals[i][mask_word] == 0\n",
    "        segment_id_vals[i][mask_word] = 1\n",
    "                \n",
    "    return new_input_mask, tf.convert_to_tensor(mask_word_ids, dtype=tf.dtypes.float32), pure_ids, segment_id_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBert(tf.keras.Model):\n",
    "    def __init__(self, class_num, bert=bert_module, dropout=0.1):\n",
    "        super(SentimentBert, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.drop1 = tf.keras.layers.Dropout(rate=dropout, trainable=True)\n",
    "        self.dense1 = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='sentiment_classification_hidden',\n",
    "            trainable=True)\n",
    "        \n",
    "        self.drop2 = tf.keras.layers.Dropout(rate=dropout, trainable=True)\n",
    "        self.dense2 = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='sentiment_classification',\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # When passed in, all tensors are stacked in one, split it into a list\n",
    "        inputs = tf.unstack(tf.cast(inputs, tf.dtypes.int32), axis=1)\n",
    "        print(inputs)\n",
    "        pooled, sequential = self.bert(inputs)\n",
    "        x = self.drop1(pooled)\n",
    "        x = self.dense1(x)\n",
    "        x = self.drop2(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Sanity test on creating and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentBert(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.trainable_weights:\n",
    "    print(weight.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(opt, loss=tf.keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 239232\n"
     ]
    }
   ],
   "source": [
    "# Get the sentiment score of each phrase and write out to a file\n",
    "DATASET_DIR = \"/home/aufish/Downloads/stanfordSentimentTreebank\"\n",
    "\n",
    "score_file = open(DATASET_DIR + \"/sentiment_labels.txt\", \"r\")\n",
    "score_dict = dict()\n",
    "\n",
    "for line in score_file.readlines():\n",
    "    parts = line.split(\"|\")\n",
    "    if parts[0] == \"phrase ids\":\n",
    "        # skip first header line\n",
    "        continue\n",
    "        \n",
    "    phrase_id, score = parts[0], float(parts[1])\n",
    "    score_dict[phrase_id] = score\n",
    "    \n",
    "score_file.close()\n",
    "\n",
    "phrase_file = open(DATASET_DIR + \"/dictionary.txt\", \"r\")\n",
    "phrase_score_file = open(\"./phrase_score.txt\", \"w\")\n",
    "\n",
    "for line in phrase_file.readlines():\n",
    "    parts = line.split(\"|\")\n",
    "    \n",
    "    score = score_dict[parts[1].strip()]\n",
    "    \n",
    "    phrase_score_file.write(\"{}|{}\\n\".format(parts[0], str(score)) )\n",
    "    \n",
    "phrase_file.close()\n",
    "phrase_score_file.close()\n",
    "\n",
    "print(\"Dataset size: {}\".format(len(score_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def parse_line(line):\n",
    "    parts = line.split(b\"|\")\n",
    "    phrase, score = parts[0].decode(), float(parts[1].decode())\n",
    "    \n",
    "    input_ids, input_mask, segment_ids = convert_sentence_to_features(phrase, tokenizer, max_seq_len=20)\n",
    "        \n",
    "    return ([input_ids, input_mask, segment_ids], score)\n",
    "\n",
    "def create_dataset(filename = \"./phrase_score.txt\", data_size=239232, batch_size=10):\n",
    "    dataset = tf.data.TextLineDataset([filename]) \n",
    "\n",
    "    dataset = dataset.map(lambda x: tf.numpy_function(parse_line, [x], [tf.int64, tf.double]))\n",
    "\n",
    "    # dataset = dataset.shuffle(data_size, reshuffle_each_iteration=False)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test what is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(20, 20), dtype=int32, numpy=\n",
      "array([[  101,   106,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   112,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   112,   112,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  2586,  2225,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   139, 11071, 13789,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   139, 11071, 13789,   106,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   139, 11071, 13789,   106,   112,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   140,   112, 19863,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  3414, 21275,   112,   188,   169,  2099,   112,\n",
      "         1110, 10965,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  2048,   117,  1440,  1120,  1115, 13336,  6341,\n",
      "          106, 11750,   117,   170,  5152,  2195,   106,   102,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  3352, 25202,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  6728,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  1109,  8275,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,  1109,  4504,   189, 26137,  1116,   106,  2048,\n",
      "          117,  1440,  1120,  1115, 13336,  6341,   106, 11750,   117,\n",
      "          170,   102],\n",
      "       [  101,   106,  7817,  4613,  5145,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106, 11750,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106, 13899,  1306,   106,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   136,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   106,   136,   112,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  101,   108,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]], dtype=int32)>, <tf.Tensor: shape=(20, 20), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32)>, <tf.Tensor: shape=(20, 20), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32)>]\n"
     ]
    }
   ],
   "source": [
    "for (bert_input, target) in dataset.take(1):\n",
    "#     print(bert_input)\n",
    "#     print(target)\n",
    "    bert_input = tf.cast(bert_input, tf.dtypes.int32)\n",
    "#     print(bert_input)\n",
    "    model(bert_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentBert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c56d6fa66c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit function has bug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentBert' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit function has bug\n",
    "model = SentimentBert(1)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(opt, loss=tf.keras.losses.BinaryCrossentropy())\n",
    "model.fit(x=dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After these trainings, the results are pretty usable\n",
    "model = SentimentBert(1)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "bce_loss= tf.keras.losses.MeanSquaredError()\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "epochs = 10\n",
    "\n",
    "dataset = create_dataset(batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(0.15566286, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.022228226, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.020681065, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.020217834, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.018260634, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.01684368, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.015902212, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.015287982, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.01478511, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.014164187, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.01379407, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.013345432, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.013089064, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.0129173165, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.012831476, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.012755842, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.012477094, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.012186626, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.011940041, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.011728749, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.011533842, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.011303672, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.011135508, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.010943471, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(0.010803446, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.010708206, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.01067937, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.01064533, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.010524017, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.010382457, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.0102497805, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.0101448605, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.010032887, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.009916561, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.009823825, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.009697607, shape=(), dtype=float32)\n",
      "Start of epoch 3\n",
      "step 0: mean loss = tf.Tensor(0.0096090445, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.009561956, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.0095458245, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.009534195, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.009455394, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.009366474, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.009291297, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.009222517, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.00914435, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.009050643, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.0089775035, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.008881483, shape=(), dtype=float32)\n",
      "Start of epoch 4\n",
      "step 0: mean loss = tf.Tensor(0.008817179, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.008785498, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.0087499125, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.008724338, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.008653224, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.008578446, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.008508423, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.008441549, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.008375646, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.008297849, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.008237872, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.0081600575, shape=(), dtype=float32)\n",
      "Start of epoch 5\n",
      "step 0: mean loss = tf.Tensor(0.008107233, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.008066676, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.008031983, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.008010898, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.007951006, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.007888744, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.00783273, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.007779456, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.007730398, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.0076668896, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.0076166554, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.007554364, shape=(), dtype=float32)\n",
      "Start of epoch 6\n",
      "step 0: mean loss = tf.Tensor(0.007507097, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.0074654017, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.007439656, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.007425138, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.007378637, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.007328153, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.0072763423, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.007231867, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.0071876533, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.0071308753, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.0070841317, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.0070322747, shape=(), dtype=float32)\n",
      "Start of epoch 7\n",
      "step 0: mean loss = tf.Tensor(0.0069907815, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.0069549647, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.0069328328, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.006915879, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.0068744626, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.006829215, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.006784817, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.0067462223, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.0067065153, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.0066593355, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.006617665, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.006573322, shape=(), dtype=float32)\n",
      "Start of epoch 8\n",
      "step 0: mean loss = tf.Tensor(0.0065359618, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.0065058055, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.0064833057, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.006464218, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.006427944, shape=(), dtype=float32)\n",
      "step 5000: mean loss = tf.Tensor(0.0063891816, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.006351399, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.0063187825, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.0062826406, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.0062427344, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.00620741, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.006168852, shape=(), dtype=float32)\n",
      "Start of epoch 9\n",
      "step 0: mean loss = tf.Tensor(0.006137747, shape=(), dtype=float32)\n",
      "step 1000: mean loss = tf.Tensor(0.0061124284, shape=(), dtype=float32)\n",
      "step 2000: mean loss = tf.Tensor(0.006091887, shape=(), dtype=float32)\n",
      "step 3000: mean loss = tf.Tensor(0.0060757375, shape=(), dtype=float32)\n",
      "step 4000: mean loss = tf.Tensor(0.006045876, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000: mean loss = tf.Tensor(0.0060133226, shape=(), dtype=float32)\n",
      "step 6000: mean loss = tf.Tensor(0.0059797843, shape=(), dtype=float32)\n",
      "step 7000: mean loss = tf.Tensor(0.0059487573, shape=(), dtype=float32)\n",
      "step 8000: mean loss = tf.Tensor(0.0059175547, shape=(), dtype=float32)\n",
      "step 9000: mean loss = tf.Tensor(0.0058818385, shape=(), dtype=float32)\n",
      "step 10000: mean loss = tf.Tensor(0.0058523146, shape=(), dtype=float32)\n",
      "step 11000: mean loss = tf.Tensor(0.0058174212, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    total_loss = 0\n",
    "    for step, (bert_input, target) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "          output = model(bert_input)\n",
    "          # Compute reconstruction loss\n",
    "          loss = bce_loss(target, output)\n",
    "          loss += sum(model.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "          print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./bert_sentiment_analysis_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_sentence_predict(model, sentence):\n",
    "    # Give a sentence and return the sentiment score of the sentence\n",
    "    input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len=20)\n",
    "    tensor_input = tf.stack([tf.constant(input_ids), tf.constant(input_mask), tf.constant(segment_ids)])\n",
    "    tensor_input = tf.reshape(tensor_input, [1, 3, 20])\n",
    "    \n",
    "    return tf.keras.backend.get_value(\n",
    "        model(tensor_input) )[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: it is a good movie, i have to admit\n",
      "Positivity score: 0.7689685821533203\n"
     ]
    }
   ],
   "source": [
    "sentence = \"it is a good movie, i have to admit\"\n",
    "score = single_sentence_predict(model, sentence)\n",
    "print(\"Input: \" + sentence)\n",
    "print(\"Positivity score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: it's pure trash\n",
      "Positivity score: 0.05330285429954529\n"
     ]
    }
   ],
   "source": [
    "sentence = \"it's pure trash\"\n",
    "score = single_sentence_predict(model, sentence)\n",
    "print(\"Input: \" + sentence)\n",
    "print(\"Positivity score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: although boring at first, it is still a genius work\n",
      "Positivity score: 0.6611613035202026\n"
     ]
    }
   ],
   "source": [
    "sentence = \"although boring at first, it is still a genius work\"\n",
    "score = single_sentence_predict(model, sentence)\n",
    "print(\"Input: \" + sentence)\n",
    "print(\"Positivity score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: no one can deny how brilliant it is\n",
      "Positivity score: 0.9334973096847534\n"
     ]
    }
   ],
   "source": [
    "sentence = \"no one can deny how brilliant it is\"\n",
    "score = single_sentence_predict(model, sentence)\n",
    "print(\"Input: \" + sentence)\n",
    "print(\"Positivity score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
