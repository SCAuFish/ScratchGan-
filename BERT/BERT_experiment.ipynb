{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading BERT and experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define functions for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"Using Tensorflow version: \" + tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "BERT_DIR = \"/home/aufish/Downloads/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try with TF2 SavedModel\n",
    "# The online downloading method does not work, use pre-downloaded module\n",
    "# bert_module = hub.Module(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\")\n",
    "\n",
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer(vocab_file, do_lower_case=False):\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer(BERT_DIR + \"/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=50):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Trial run for methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I prefer Python over Java', 'I love ice cream the best']\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = convert_sentences_to_features(sentences, tokenizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 1 in mask\n",
    "bert_inputs = [input_ids_vals, input_mask_vals, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_vals)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value for mask of one word\n",
    "import copy\n",
    "\n",
    "input_mask_val_2 = copy.deepcopy(input_mask_vals)\n",
    "input_mask_val_2[0][0] = 0\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_mask_val_2, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_val_2)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create classifier model keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add a layer to define predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictor(tf.keras.Model):\n",
    "    def __init__(self, bert_layer, class_num, drop_out=0.1):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.bert = bert_layer\n",
    "        self.drop = tf.keras.layers.Dropout(rate=drop_out)\n",
    "        self.dense= tf.keras.layers.Dense(\n",
    "            class_num,\n",
    "            activation=None,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='predictions/transform/logits')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        pooled, sequential = self.bert(inputs)\n",
    "        x = self.drop(pooled)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sanity test for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model(bert_inputs)\n",
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train BERT for Masked-word Predition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Util function to randomly mask a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "def make_rand_mask(input_ids, input_mask, vocab_size, segment_id_vals=None):\n",
    "    ''' \n",
    "    input_ids: the ids of words in the sentences\n",
    "    input_mask: initial mask (1 if there is a word; 0 for padding)\n",
    "    returns\n",
    "    input_mask: replace one bit of 1 with 0, meaning that the word will be masked\n",
    "    mask_word_ids: the id of words that are masked\n",
    "    pure_ids: ids in number instead of one-hot (to generate weights per masked word)\n",
    "    segment_id_vals: mark the masked word with segment id 1\n",
    "    '''\n",
    "    batch_size = len(input_ids)\n",
    "    \n",
    "    new_input_mask = copy.deepcopy(input_mask)\n",
    "    mask_word_ids = np.zeros((batch_size, vocab_size))\n",
    "    pure_ids = []\n",
    "    segment_encodings = []\n",
    "    for i in range(batch_size):\n",
    "        total_word = sum(input_mask[i])\n",
    "        mask_word = random.randint(0, total_word-1)\n",
    "        \n",
    "        pure_ids.append(input_ids[i][mask_word])\n",
    "        assert new_input_mask[i][mask_word] == 1\n",
    "        new_input_mask[i][mask_word] = 0\n",
    "        mask_word_ids[i][input_ids[i][mask_word]] = 1.0\n",
    "        \n",
    "        # Make the masked word segment id 1\n",
    "        assert segment_id_vals[i][mask_word] == 0\n",
    "        segment_id_vals[i][mask_word] = 1\n",
    "                \n",
    "    return new_input_mask, tf.convert_to_tensor(mask_word_ids, dtype=tf.dtypes.float32), pure_ids, segment_id_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_masks, labels, _, segment_ids_vals = make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "result = model(bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "total_loss = 0\n",
    "with tf.GradientTape() as tape:\n",
    "    result = model(bert_inputs)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels, result)\n",
    "    total_loss += loss\n",
    "grads = tape.gradient(loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(grads) == len(model.trainable_weights)\n",
    "print(tf.reduce_sum(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load data from ScratchGan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sources have suggested that so far the company sees no reason to change its tax structures , which are perfectly legal .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_FILE = \"/home/aufish/Documents/ScratchGan++/scratchgan/emnlp_data/train.json\"\n",
    "all_sentences = json.load(open(DATA_FILE, \"r\"))\n",
    "\n",
    "all_sentences = [sentence['s'] for sentence in all_sentences]\n",
    "print(all_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mitigate unbalanced weights, count different words\n",
    "from collections import defaultdict\n",
    "\n",
    "count = defaultdict(int)\n",
    "\n",
    "max_id, max_count = 0, 0\n",
    "total_count = 0\n",
    "for sentence in all_sentences:\n",
    "    ids, _, _ = convert_sentences_to_features([sentence], tokenizer)\n",
    "    id_list = ids[0]\n",
    "    for id in id_list:\n",
    "        count[id] += 1\n",
    "        total_count += 1\n",
    "        if count[id] > max_count:\n",
    "            max_id = id\n",
    "            max_count = count[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 268586\n",
      "Number of words: 13429300\n",
      "Most frequent id: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Data size: {}\".format(len(all_sentences)))\n",
    "print(\"Number of words: {}\".format(total_count))\n",
    "print(\"Most frequent id: {}\".format(max_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_predictor(model, all_sentences, tokenizer, batch_size = 1, epoch = 1):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    data_size   = len(all_sentences)\n",
    "    print(\"Data size: {}\".format(data_size))\n",
    "    for i in range(epoch):\n",
    "        print(\"Starting epoch {}\".format(i))\n",
    "        \n",
    "        # Use the first 1000 sentences for evaluation\n",
    "        start_index = 0\n",
    "        end_index   = start_index + batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        while start_index < data_size:\n",
    "            if start_index % 100 == 0:\n",
    "                print(\"Sentence index: {}\\r\".format(start_index))\n",
    "                \n",
    "                print(\"Total loss: {}\".format(total_loss))\n",
    "                total_loss = 0\n",
    "\n",
    "                accuracy = eval_accuracy(model, all_sentences[:1000], tokenizer)\n",
    "                print(\"Accuracy: {}\".format(accuracy))\n",
    "            end_index = min(data_size, start_index + batch_size)\n",
    "            \n",
    "            input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "                convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "            \n",
    "            input_masks, labels, masked_ids, segment_ids_vals = \\\n",
    "                make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "    \n",
    "            bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "            weights = tf.convert_to_tensor(np.array([1 / count[id] for id in masked_ids]))\n",
    "    \n",
    "            with tf.GradientTape() as tape:\n",
    "                result = model(bert_inputs)\n",
    "                loss = tf.compat.v1.losses.softmax_cross_entropy(labels, result, weights=weights)\n",
    "                total_loss += tf.reduce_sum(loss)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            \n",
    "            opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])\n",
    "            \n",
    "            start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, all_sentences, tokenizer):\n",
    "    correct_num = 0\n",
    "    start_index = 0\n",
    "    batch_size = 10\n",
    "    end_index  = start_index + batch_size\n",
    "    \n",
    "    while start_index < len(all_sentences):\n",
    "        end_index = min(len(all_sentences), start_index + batch_size)\n",
    "        input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "            convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "\n",
    "        input_masks, labels, _, segment_ids_vals = \\\n",
    "            make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "        bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "        result = model(bert_inputs)\n",
    "\n",
    "        model_choices = tf.argmax(result, axis=1)\n",
    "        labels = tf.argmax(labels, axis=1)\n",
    "\n",
    "        equal_result = tf.equal(model_choices, labels)\n",
    "        correct_num += tf.reduce_sum(tf.cast(equal_result , tf.int32))\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    return correct_num / len(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1000\n",
      "Starting epoch 0\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.0\n",
      "Sentence index: 100\n",
      "Total loss: 0.07267780601978302\n",
      "Accuracy: 0.0\n",
      "Sentence index: 200\n",
      "Total loss: 0.059415657073259354\n",
      "Accuracy: 0.0\n",
      "Sentence index: 300\n",
      "Total loss: 0.06780267506837845\n",
      "Accuracy: 0.0\n",
      "Sentence index: 400\n",
      "Total loss: 0.058475032448768616\n",
      "Accuracy: 0.0\n",
      "Sentence index: 500\n",
      "Total loss: 0.09598041325807571\n",
      "Accuracy: 0.0\n",
      "Sentence index: 600\n",
      "Total loss: 0.04817742109298706\n",
      "Accuracy: 0.0\n",
      "Sentence index: 700\n",
      "Total loss: 0.07618837058544159\n",
      "Accuracy: 0.0\n",
      "Sentence index: 800\n",
      "Total loss: 0.0571945458650589\n",
      "Accuracy: 0.0\n",
      "Sentence index: 900\n",
      "Total loss: 0.05145822465419769\n",
      "Accuracy: 0.0\n",
      "Starting epoch 1\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.0\n",
      "Sentence index: 100\n",
      "Total loss: 0.11498606204986572\n",
      "Accuracy: 0.0\n",
      "Sentence index: 200\n",
      "Total loss: 0.04866473749279976\n",
      "Accuracy: 0.0\n",
      "Sentence index: 300\n",
      "Total loss: 0.05270647257566452\n",
      "Accuracy: 0.0\n",
      "Sentence index: 400\n",
      "Total loss: 0.060168977826833725\n",
      "Accuracy: 0.0\n",
      "Sentence index: 500\n",
      "Total loss: 0.1038535013794899\n",
      "Accuracy: 0.0\n",
      "Sentence index: 600\n",
      "Total loss: 0.04958558455109596\n",
      "Accuracy: 0.0\n",
      "Sentence index: 700\n",
      "Total loss: 0.043915003538131714\n",
      "Accuracy: 0.0\n",
      "Sentence index: 800\n",
      "Total loss: 0.054765596985816956\n",
      "Accuracy: 0.0\n",
      "Sentence index: 900\n",
      "Total loss: 0.10329931229352951\n",
      "Accuracy: 0.0\n",
      "Starting epoch 2\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.0\n",
      "Sentence index: 100\n",
      "Total loss: 0.10323144495487213\n",
      "Accuracy: 0.0\n",
      "Sentence index: 200\n",
      "Total loss: 0.053843483328819275\n",
      "Accuracy: 0.0\n",
      "Sentence index: 300\n",
      "Total loss: 0.06763476878404617\n",
      "Accuracy: 0.0\n",
      "Sentence index: 400\n",
      "Total loss: 0.05736947059631348\n",
      "Accuracy: 0.0\n",
      "Sentence index: 500\n",
      "Total loss: 0.07099240273237228\n",
      "Accuracy: 0.0\n",
      "Sentence index: 600\n",
      "Total loss: 0.054098840802907944\n",
      "Accuracy: 0.0\n",
      "Sentence index: 700\n",
      "Total loss: 0.07777946442365646\n",
      "Accuracy: 0.0\n",
      "Sentence index: 800\n",
      "Total loss: 0.06821063160896301\n",
      "Accuracy: 0.001\n",
      "Sentence index: 900\n",
      "Total loss: 0.038459308445453644\n",
      "Accuracy: 0.0\n",
      "Starting epoch 3\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.001\n",
      "Sentence index: 100\n",
      "Total loss: 0.06102634221315384\n",
      "Accuracy: 0.0\n",
      "Sentence index: 200\n",
      "Total loss: 0.09374575316905975\n",
      "Accuracy: 0.0\n",
      "Sentence index: 300\n",
      "Total loss: 0.05105763301253319\n",
      "Accuracy: 0.003\n",
      "Sentence index: 400\n",
      "Total loss: 0.052637290209531784\n",
      "Accuracy: 0.001\n",
      "Sentence index: 500\n",
      "Total loss: 0.07959806174039841\n",
      "Accuracy: 0.001\n",
      "Sentence index: 600\n",
      "Total loss: 0.046331875026226044\n",
      "Accuracy: 0.001\n",
      "Sentence index: 700\n",
      "Total loss: 0.06019607186317444\n",
      "Accuracy: 0.0\n",
      "Sentence index: 800\n",
      "Total loss: 0.09335215389728546\n",
      "Accuracy: 0.001\n",
      "Sentence index: 900\n",
      "Total loss: 0.042041584849357605\n",
      "Accuracy: 0.0\n",
      "Starting epoch 4\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.0\n",
      "Sentence index: 100\n",
      "Total loss: 0.034496504813432693\n",
      "Accuracy: 0.002\n",
      "Sentence index: 200\n",
      "Total loss: 0.05446172505617142\n",
      "Accuracy: 0.0\n",
      "Sentence index: 300\n",
      "Total loss: 0.05352308228611946\n",
      "Accuracy: 0.0\n",
      "Sentence index: 400\n",
      "Total loss: 0.05535240098834038\n",
      "Accuracy: 0.001\n",
      "Sentence index: 500\n",
      "Total loss: 0.08859739452600479\n",
      "Accuracy: 0.008\n",
      "Sentence index: 600\n",
      "Total loss: 0.08626927435398102\n",
      "Accuracy: 0.019\n",
      "Sentence index: 700\n",
      "Total loss: 0.05784273520112038\n",
      "Accuracy: 0.026\n",
      "Sentence index: 800\n",
      "Total loss: 0.0813186913728714\n",
      "Accuracy: 0.026\n",
      "Sentence index: 900\n",
      "Total loss: 0.06654805690050125\n",
      "Accuracy: 0.027\n",
      "Starting epoch 5\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.025\n",
      "Sentence index: 100\n",
      "Total loss: 0.08991582691669464\n",
      "Accuracy: 0.032\n",
      "Sentence index: 200\n",
      "Total loss: 0.09129726141691208\n",
      "Accuracy: 0.029\n",
      "Sentence index: 300\n",
      "Total loss: 0.067470483481884\n",
      "Accuracy: 0.029\n",
      "Sentence index: 400\n",
      "Total loss: 0.053673744201660156\n",
      "Accuracy: 0.03\n",
      "Sentence index: 500\n",
      "Total loss: 0.055923543870449066\n",
      "Accuracy: 0.031\n",
      "Sentence index: 600\n",
      "Total loss: 0.08952441811561584\n",
      "Accuracy: 0.014\n",
      "Sentence index: 700\n",
      "Total loss: 0.07776675373315811\n",
      "Accuracy: 0.029\n",
      "Sentence index: 800\n",
      "Total loss: 0.07270842790603638\n",
      "Accuracy: 0.032\n",
      "Sentence index: 900\n",
      "Total loss: 0.06450161337852478\n",
      "Accuracy: 0.022\n",
      "Starting epoch 6\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.026\n",
      "Sentence index: 100\n",
      "Total loss: 0.07964751869440079\n",
      "Accuracy: 0.027\n",
      "Sentence index: 200\n",
      "Total loss: 0.07290760427713394\n",
      "Accuracy: 0.036\n",
      "Sentence index: 300\n",
      "Total loss: 0.06287577003240585\n",
      "Accuracy: 0.032\n",
      "Sentence index: 400\n",
      "Total loss: 0.06978094577789307\n",
      "Accuracy: 0.035\n",
      "Sentence index: 500\n",
      "Total loss: 0.07895752042531967\n",
      "Accuracy: 0.033\n",
      "Sentence index: 600\n",
      "Total loss: 0.0667494386434555\n",
      "Accuracy: 0.022\n",
      "Sentence index: 700\n",
      "Total loss: 0.07345088571310043\n",
      "Accuracy: 0.032\n",
      "Sentence index: 800\n",
      "Total loss: 0.07101327180862427\n",
      "Accuracy: 0.011\n",
      "Sentence index: 900\n",
      "Total loss: 0.049490757286548615\n",
      "Accuracy: 0.025\n",
      "Starting epoch 7\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.024\n",
      "Sentence index: 100\n",
      "Total loss: 0.06405070424079895\n",
      "Accuracy: 0.029\n",
      "Sentence index: 200\n",
      "Total loss: 0.0725308358669281\n",
      "Accuracy: 0.029\n",
      "Sentence index: 300\n",
      "Total loss: 0.08032733201980591\n",
      "Accuracy: 0.023\n",
      "Sentence index: 400\n",
      "Total loss: 0.059767015278339386\n",
      "Accuracy: 0.032\n",
      "Sentence index: 500\n",
      "Total loss: 0.08148250728845596\n",
      "Accuracy: 0.025\n",
      "Sentence index: 600\n",
      "Total loss: 0.06206296384334564\n",
      "Accuracy: 0.023\n",
      "Sentence index: 700\n",
      "Total loss: 0.0476403534412384\n",
      "Accuracy: 0.042\n",
      "Sentence index: 800\n",
      "Total loss: 0.08716137707233429\n",
      "Accuracy: 0.04\n",
      "Sentence index: 900\n",
      "Total loss: 0.06763702630996704\n",
      "Accuracy: 0.035\n",
      "Starting epoch 8\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.032\n",
      "Sentence index: 100\n",
      "Total loss: 0.043523773550987244\n",
      "Accuracy: 0.027\n",
      "Sentence index: 200\n",
      "Total loss: 0.043027013540267944\n",
      "Accuracy: 0.033\n",
      "Sentence index: 300\n",
      "Total loss: 0.07282590121030807\n",
      "Accuracy: 0.03\n",
      "Sentence index: 400\n",
      "Total loss: 0.06024660915136337\n",
      "Accuracy: 0.027\n",
      "Sentence index: 500\n",
      "Total loss: 0.03746197745203972\n",
      "Accuracy: 0.029\n",
      "Sentence index: 600\n",
      "Total loss: 0.11310369521379471\n",
      "Accuracy: 0.024\n",
      "Sentence index: 700\n",
      "Total loss: 0.04257393628358841\n",
      "Accuracy: 0.018\n",
      "Sentence index: 800\n",
      "Total loss: 0.09210168570280075\n",
      "Accuracy: 0.03\n",
      "Sentence index: 900\n",
      "Total loss: 0.06793025135993958\n",
      "Accuracy: 0.029\n",
      "Starting epoch 9\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.033\n",
      "Sentence index: 100\n",
      "Total loss: 0.054971423000097275\n",
      "Accuracy: 0.025\n",
      "Sentence index: 200\n",
      "Total loss: 0.05342324450612068\n",
      "Accuracy: 0.04\n",
      "Sentence index: 300\n",
      "Total loss: 0.07121157646179199\n",
      "Accuracy: 0.031\n",
      "Sentence index: 400\n",
      "Total loss: 0.1093350276350975\n",
      "Accuracy: 0.026\n",
      "Sentence index: 500\n",
      "Total loss: 0.06992914527654648\n",
      "Accuracy: 0.027\n",
      "Sentence index: 600\n",
      "Total loss: 0.05373253673315048\n",
      "Accuracy: 0.039\n",
      "Sentence index: 700\n",
      "Total loss: 0.06090172380208969\n",
      "Accuracy: 0.026\n",
      "Sentence index: 800\n",
      "Total loss: 0.06872522830963135\n",
      "Accuracy: 0.027\n",
      "Sentence index: 900\n",
      "Total loss: 0.06624656915664673\n",
      "Accuracy: 0.029\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "\n",
    "train_word_predictor(model, all_sentences[:1000], tokenizer, batch_size=10, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./trained_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f714c581f98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model2 = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model.load_weights(\"./trained_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Qualitatively check predicting result of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [\"My sources have suggested that so far the company sees no reason to change its tax structures , which are perfectly legal .\"]\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "    convert_sentences_to_features(all_sentences, tokenizer)\n",
    "\n",
    "input_masks, labels, _, segment_ids_vals = \\\n",
    "    make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "result = model(bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "tf.Tensor([4413], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(input_masks)\n",
    "print(tf.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word: ['structures']\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing word: {}\".format(tokenizer.convert_ids_to_tokens([4413])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=\n",
       "array([1106, 1128, 1234, 1103,  170,  101, 1122, 1104, 1121, 1111],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(result, axis=1, direction=\"DESCENDING\")[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.1318012, shape=(), dtype=float32)\n",
      "['to']\n",
      "tf.Tensor(1.0151392, shape=(), dtype=float32)\n",
      "['you']\n",
      "tf.Tensor(0.9961909, shape=(), dtype=float32)\n",
      "['people']\n",
      "tf.Tensor(0.94377273, shape=(), dtype=float32)\n",
      "['the']\n",
      "tf.Tensor(0.9264552, shape=(), dtype=float32)\n",
      "['a']\n",
      "tf.Tensor(0.9176497, shape=(), dtype=float32)\n",
      "['[CLS]']\n",
      "tf.Tensor(0.9131078, shape=(), dtype=float32)\n",
      "['it']\n",
      "tf.Tensor(0.90673125, shape=(), dtype=float32)\n",
      "['of']\n",
      "tf.Tensor(0.9033523, shape=(), dtype=float32)\n",
      "['for']\n",
      "tf.Tensor(0.9031592, shape=(), dtype=float32)\n",
      "['from']\n"
     ]
    }
   ],
   "source": [
    "candidates = [1106, 1128, 1234, 1103,  170,  101, 1122, 1104, 1111, 1121]\n",
    "for candidate in candidates:\n",
    "    print(result[0, candidate])\n",
    "    print(tokenizer.convert_ids_to_tokens([candidate]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
