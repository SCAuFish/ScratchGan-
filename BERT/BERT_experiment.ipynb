{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading BERT and experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define functions for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"Using Tensorflow version: \" + tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "BERT_DIR = \"/home/aufish/Downloads/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try with TF2 SavedModel\n",
    "# The online downloading method does not work, use pre-downloaded module\n",
    "# bert_module = hub.Module(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\")\n",
    "\n",
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer(vocab_file, do_lower_case=False):\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer(BERT_DIR + \"/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are actually not used\n",
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=50):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Trial run for methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I prefer Python over Java', 'I love ice cream the best']\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = convert_sentences_to_features(sentences, tokenizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 146, 9353, 23334, 1166, 9155, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 146, 1567, 2854, 7081, 1103, 1436, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "tf.Tensor(\n",
      "[[-0.8131681   0.5370047   0.99996513 ...  0.999987   -0.5378665\n",
      "   0.9919909 ]\n",
      " [-0.60621274  0.49540225  0.99991035 ...  0.9999771  -0.91028047\n",
      "   0.9931733 ]], shape=(2, 768), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 0.8713086   0.00939399  0.02123968 ... -0.2102821   0.45887846\n",
      "    0.23217922]\n",
      "  [ 0.5732106  -0.13002536  0.33161387 ...  0.05539769 -0.10422937\n",
      "    0.3469553 ]\n",
      "  [ 0.42146546 -0.2859141  -0.23261456 ...  0.25038135 -0.60215116\n",
      "    0.20282936]\n",
      "  ...\n",
      "  [ 0.37351245  0.26639333  0.43967193 ... -0.1845916   0.40790993\n",
      "    0.4635392 ]\n",
      "  [ 0.38079703  0.18587998  0.27819872 ... -0.21876818  0.43730605\n",
      "    0.39033228]\n",
      "  [ 0.28391755  0.11993123  0.34593338 ...  0.01512105  0.33790368\n",
      "    0.16361022]]\n",
      "\n",
      " [[ 0.49074587  0.160682    0.03785331 ... -0.23446186  0.2617393\n",
      "    0.0903241 ]\n",
      "  [ 0.13407     0.2104078   0.25069734 ...  0.23977542 -0.05616882\n",
      "    0.32233608]\n",
      "  [-0.05624516  0.01800643 -0.16432334 ...  0.65063506 -1.0328202\n",
      "    0.10341886]\n",
      "  ...\n",
      "  [ 0.05746117  0.23868068  0.09878956 ...  0.37608162 -0.19921656\n",
      "   -0.34361216]\n",
      "  [-0.01977242  0.24953367  0.2952392  ...  0.15219964 -0.02327026\n",
      "   -0.05462281]\n",
      "  [-0.13560185  0.38314173  0.36137334 ...  0.15484728 -0.12740727\n",
      "   -0.18449263]]], shape=(2, 20, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# all 1 in mask\n",
    "bert_inputs = [input_ids_vals, input_mask_vals, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_vals)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[ 0.42146546, -0.2859141 , -0.23261456, ...,  0.25038135,\n",
       "        -0.60215116,  0.20282936],\n",
       "       [-0.5769223 , -0.00628331,  0.02436075, ...,  0.24750945,\n",
       "         0.01002553,  0.13241625]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = out[1]\n",
    "tf.gather_nd(s, [[0, 2], [1, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value for mask of one word\n",
    "import copy\n",
    "\n",
    "input_mask_val_2 = copy.deepcopy(input_mask_vals)\n",
    "input_mask_val_2[0][0] = 0\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_mask_val_2, segment_ids_vals]\n",
    "\n",
    "print(input_ids_vals)\n",
    "print(input_mask_val_2)\n",
    "print(segment_ids_vals)\n",
    "\n",
    "out = bert_module (bert_inputs)\n",
    "\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create classifier model keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add a layer to define predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictor(tf.keras.Model):\n",
    "    def __init__(self, bert_layer, class_num, drop_out=0.1):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.bert = bert_layer\n",
    "        self.drop1 = tf.keras.layers.Dropout(rate=drop_out)\n",
    "        self.dense1= tf.keras.layers.Dense(\n",
    "            768,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='predictions/transform/hidden')\n",
    "        \n",
    "        self.drop2 = tf.keras.layers.Dropout(rate=drop_out)\n",
    "        self.dense2 = tf.keras.layers.Dense(\n",
    "            class_num,\n",
    "            activation = None, \n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='predictions/transform/final')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        \n",
    "        # masked_word_ids should be in the format [ [sentence_id, word_id], ... ]\n",
    "        bert_layer_input, masked_word_ids = inputs\n",
    "        pooled, sequential = self.bert(bert_layer_input)\n",
    "        \n",
    "        x = tf.gather_nd(sequential, masked_word_ids)\n",
    "        # use sequential instead of pool\n",
    "        x = self.drop1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.drop2(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sanity test for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "bert_model/word_embeddings/embeddings:0\n",
      "bert_model/embedding_postprocessor/type_embeddings:0\n",
      "bert_model/embedding_postprocessor/position_embeddings:0\n",
      "bert_model/embedding_postprocessor/layer_norm/gamma:0\n",
      "bert_model/embedding_postprocessor/layer_norm/beta:0\n",
      "bert_model/encoder/layer_0/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_0/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_0/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_0/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_0/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_0/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_0/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_0/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_0/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_0/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_0/intermediate/kernel:0\n",
      "bert_model/encoder/layer_0/intermediate/bias:0\n",
      "bert_model/encoder/layer_0/output/kernel:0\n",
      "bert_model/encoder/layer_0/output/bias:0\n",
      "bert_model/encoder/layer_0/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_0/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_1/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_1/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_1/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_1/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_1/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_1/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_1/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_1/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_1/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_1/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_1/intermediate/kernel:0\n",
      "bert_model/encoder/layer_1/intermediate/bias:0\n",
      "bert_model/encoder/layer_1/output/kernel:0\n",
      "bert_model/encoder/layer_1/output/bias:0\n",
      "bert_model/encoder/layer_1/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_1/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_2/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_2/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_2/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_2/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_2/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_2/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_2/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_2/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_2/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_2/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_2/intermediate/kernel:0\n",
      "bert_model/encoder/layer_2/intermediate/bias:0\n",
      "bert_model/encoder/layer_2/output/kernel:0\n",
      "bert_model/encoder/layer_2/output/bias:0\n",
      "bert_model/encoder/layer_2/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_2/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_3/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_3/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_3/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_3/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_3/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_3/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_3/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_3/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_3/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_3/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_3/intermediate/kernel:0\n",
      "bert_model/encoder/layer_3/intermediate/bias:0\n",
      "bert_model/encoder/layer_3/output/kernel:0\n",
      "bert_model/encoder/layer_3/output/bias:0\n",
      "bert_model/encoder/layer_3/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_3/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_4/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_4/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_4/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_4/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_4/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_4/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_4/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_4/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_4/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_4/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_4/intermediate/kernel:0\n",
      "bert_model/encoder/layer_4/intermediate/bias:0\n",
      "bert_model/encoder/layer_4/output/kernel:0\n",
      "bert_model/encoder/layer_4/output/bias:0\n",
      "bert_model/encoder/layer_4/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_4/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_5/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_5/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_5/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_5/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_5/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_5/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_5/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_5/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_5/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_5/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_5/intermediate/kernel:0\n",
      "bert_model/encoder/layer_5/intermediate/bias:0\n",
      "bert_model/encoder/layer_5/output/kernel:0\n",
      "bert_model/encoder/layer_5/output/bias:0\n",
      "bert_model/encoder/layer_5/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_5/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_6/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_6/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_6/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_6/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_6/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_6/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_6/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_6/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_6/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_6/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_6/intermediate/kernel:0\n",
      "bert_model/encoder/layer_6/intermediate/bias:0\n",
      "bert_model/encoder/layer_6/output/kernel:0\n",
      "bert_model/encoder/layer_6/output/bias:0\n",
      "bert_model/encoder/layer_6/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_6/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_7/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_7/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_7/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_7/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_7/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_7/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_7/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_7/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_7/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_7/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_7/intermediate/kernel:0\n",
      "bert_model/encoder/layer_7/intermediate/bias:0\n",
      "bert_model/encoder/layer_7/output/kernel:0\n",
      "bert_model/encoder/layer_7/output/bias:0\n",
      "bert_model/encoder/layer_7/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_7/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_8/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_8/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_8/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_8/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_8/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_8/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_8/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_8/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_8/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_8/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_8/intermediate/kernel:0\n",
      "bert_model/encoder/layer_8/intermediate/bias:0\n",
      "bert_model/encoder/layer_8/output/kernel:0\n",
      "bert_model/encoder/layer_8/output/bias:0\n",
      "bert_model/encoder/layer_8/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_8/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_9/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_9/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_9/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_9/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_9/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_9/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_9/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_9/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_9/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_9/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_9/intermediate/kernel:0\n",
      "bert_model/encoder/layer_9/intermediate/bias:0\n",
      "bert_model/encoder/layer_9/output/kernel:0\n",
      "bert_model/encoder/layer_9/output/bias:0\n",
      "bert_model/encoder/layer_9/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_9/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_10/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_10/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_10/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_10/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_10/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_10/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_10/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_10/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_10/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_10/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_10/intermediate/kernel:0\n",
      "bert_model/encoder/layer_10/intermediate/bias:0\n",
      "bert_model/encoder/layer_10/output/kernel:0\n",
      "bert_model/encoder/layer_10/output/bias:0\n",
      "bert_model/encoder/layer_10/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_10/output_layer_norm/beta:0\n",
      "bert_model/encoder/layer_11/self_attention/query/kernel:0\n",
      "bert_model/encoder/layer_11/self_attention/query/bias:0\n",
      "bert_model/encoder/layer_11/self_attention/key/kernel:0\n",
      "bert_model/encoder/layer_11/self_attention/key/bias:0\n",
      "bert_model/encoder/layer_11/self_attention/value/kernel:0\n",
      "bert_model/encoder/layer_11/self_attention/value/bias:0\n",
      "bert_model/encoder/layer_11/self_attention_output/kernel:0\n",
      "bert_model/encoder/layer_11/self_attention_output/bias:0\n",
      "bert_model/encoder/layer_11/self_attention_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_11/self_attention_layer_norm/beta:0\n",
      "bert_model/encoder/layer_11/intermediate/kernel:0\n",
      "bert_model/encoder/layer_11/intermediate/bias:0\n",
      "bert_model/encoder/layer_11/output/kernel:0\n",
      "bert_model/encoder/layer_11/output/bias:0\n",
      "bert_model/encoder/layer_11/output_layer_norm/gamma:0\n",
      "bert_model/encoder/layer_11/output_layer_norm/beta:0\n",
      "bert_model/pooler_transform/kernel:0\n",
      "bert_model/pooler_transform/bias:0\n",
      "predictions/transform/hidden/kernel:0\n",
      "predictions/transform/hidden/bias:0\n",
      "predictions/transform/final/kernel:0\n",
      "predictions/transform/final/bias:0\n"
     ]
    }
   ],
   "source": [
    "# Sanity test\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "\n",
    "input_with_id = [bert_inputs, [[i, 0] for i in range(len(bert_inputs))]]\n",
    "print(len(input_with_id))\n",
    "model(input_with_id)\n",
    "for weight in model.trainable_weights:\n",
    "    print(weight.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train BERT for Masked-word Predition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Util function to randomly mask a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "def make_rand_mask(input_ids, input_mask, vocab_size, segment_id_vals=None):\n",
    "    ''' \n",
    "    input_ids: the ids of words in the sentences\n",
    "    input_mask: initial mask (1 if there is a word; 0 for padding)\n",
    "    returns\n",
    "    input_mask: replace one bit of 1 with 0, meaning that the word will be masked\n",
    "    mask_word_ids: the id of words that are masked\n",
    "    pure_ids: ids in number instead of one-hot (to generate weights per masked word)\n",
    "    segment_id_vals: mark the masked word with segment id 1\n",
    "    '''\n",
    "    batch_size = len(input_ids)\n",
    "    \n",
    "    new_input_mask = copy.deepcopy(input_mask)\n",
    "    mask_word_ids = np.zeros((batch_size, vocab_size))\n",
    "    pure_ids = []\n",
    "    segment_encodings = []\n",
    "    for i in range(batch_size):\n",
    "        total_word = sum(input_mask[i])\n",
    "        mask_word = random.randint(0, total_word-1)\n",
    "        \n",
    "        pure_ids.append(input_ids[i][mask_word])\n",
    "        assert new_input_mask[i][mask_word] == 1\n",
    "        new_input_mask[i][mask_word] = 0\n",
    "        mask_word_ids[i][input_ids[i][mask_word]] = 1.0\n",
    "        \n",
    "        # Make the masked word segment id 1\n",
    "        # assert segment_id_vals[i][mask_word] == 0\n",
    "        # segment_id_vals[i][mask_word] = 1\n",
    "                \n",
    "    return new_input_mask, tf.convert_to_tensor(mask_word_ids, dtype=tf.dtypes.float32), pure_ids, segment_id_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_masks, labels, pure_ids, segment_ids_vals = make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "masked_word_filter = [[i, pure_ids[i]] for i in range(len(pure_ids))]\n",
    "result = model( [bert_inputs, masked_word_filter] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "total_loss = 0\n",
    "with tf.GradientTape() as tape:\n",
    "    result = model(bert_inputs)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels, result)\n",
    "    total_loss += loss\n",
    "grads = tape.gradient(loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(grads) == len(model.trainable_weights)\n",
    "print(tf.reduce_sum(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.apply_gradients([(grads[i], model.trainable_weights[i]) for i in range(len(grads))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load data from ScratchGan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sources have suggested that so far the company sees no reason to change its tax structures , which are perfectly legal .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_FILE = \"/home/aufish/Documents/ScratchGan++/scratchgan/emnlp_data/train.json\"\n",
    "all_sentences = json.load(open(DATA_FILE, \"r\"))\n",
    "\n",
    "all_sentences = [sentence['s'] for sentence in all_sentences]\n",
    "print(all_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mitigate unbalanced weights, count different words\n",
    "from collections import defaultdict\n",
    "\n",
    "count = defaultdict(int)\n",
    "\n",
    "max_id, max_count = 0, 0\n",
    "total_count = 0\n",
    "for sentence in all_sentences:\n",
    "    ids, _, _ = convert_sentences_to_features([sentence], tokenizer)\n",
    "    id_list = ids[0]\n",
    "    for id in id_list:\n",
    "        count[id] += 1\n",
    "        total_count += 1\n",
    "        if count[id] > max_count:\n",
    "            max_id = id\n",
    "            max_count = count[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 268586\n",
      "Number of words: 13429300\n",
      "Most frequent id: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Data size: {}\".format(len(all_sentences)))\n",
    "print(\"Number of words: {}\".format(total_count))\n",
    "print(\"Most frequent id: {}\".format(max_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_predictor(model, all_sentences, tokenizer, batch_size = 1, epoch = 1):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    data_size   = len(all_sentences)\n",
    "    print(\"Data size: {}\".format(data_size))\n",
    "    for i in range(epoch):\n",
    "        print(\"Starting epoch {}\".format(i))\n",
    "        \n",
    "        # Use the first 1000 sentences for evaluation\n",
    "        start_index = 0\n",
    "        end_index   = start_index + batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        while start_index < data_size:\n",
    "            if start_index % 1000 == 0:\n",
    "                print(\"Sentence index: {}\\r\".format(start_index))\n",
    "                \n",
    "                print(\"Total loss: {}\".format(total_loss))\n",
    "                total_loss = 0\n",
    "\n",
    "                accuracy = eval_accuracy(model, all_sentences[:1000], tokenizer)\n",
    "                print(\"Accuracy: {}\".format(accuracy))\n",
    "            end_index = min(data_size, start_index + batch_size)\n",
    "            \n",
    "            input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "                convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "            \n",
    "            input_masks, labels, masked_ids, segment_ids_vals = \\\n",
    "                make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "    \n",
    "            bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "            weights = tf.convert_to_tensor(np.array([1 / count[id] for id in masked_ids]))\n",
    "    \n",
    "            masked_word_filter = [[i, masked_ids[i]] for i in range(len(masked_ids))]\n",
    "            with tf.GradientTape() as tape:\n",
    "                result = model( (bert_inputs, masked_word_filter) )\n",
    "                loss = tf.compat.v1.losses.softmax_cross_entropy(labels, result, weights=weights)\n",
    "                total_loss += tf.reduce_sum(loss)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            \n",
    "            opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, all_sentences, tokenizer):\n",
    "    correct_num = 0\n",
    "    start_index = 0\n",
    "    batch_size = 10\n",
    "    end_index  = start_index + batch_size\n",
    "    \n",
    "    while start_index < len(all_sentences):\n",
    "        end_index = min(len(all_sentences), start_index + batch_size)\n",
    "        input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "            convert_sentences_to_features(all_sentences[start_index:end_index], tokenizer)\n",
    "\n",
    "        input_masks, labels, pure_ids, segment_ids_vals = \\\n",
    "            make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "        masked_word_filter = [[i, pure_ids[i]] for i in range(len(pure_ids))]\n",
    "        bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "\n",
    "        result = model( (bert_inputs, masked_word_filter) )\n",
    "\n",
    "        model_choices = tf.argmax(result, axis=1)\n",
    "        labels = tf.argmax(labels, axis=1)\n",
    "\n",
    "        equal_result = tf.equal(model_choices, labels)\n",
    "        correct_num += tf.reduce_sum(tf.cast(equal_result , tf.int32))\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    return correct_num / len(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 268586\n",
      "Starting epoch 0\n",
      "Sentence index: 0\n",
      "Total loss: 0\n",
      "Accuracy: 0.0\n",
      "Sentence index: 1000\n",
      "Total loss: 0.7254385948181152\n",
      "Accuracy: 0.02\n",
      "Sentence index: 2000\n",
      "Total loss: 0.8310202360153198\n",
      "Accuracy: 0.019\n",
      "Sentence index: 3000\n",
      "Total loss: 0.7174498438835144\n",
      "Accuracy: 0.034\n",
      "Sentence index: 4000\n",
      "Total loss: 0.706411600112915\n",
      "Accuracy: 0.021\n",
      "Sentence index: 5000\n",
      "Total loss: 0.7249010801315308\n",
      "Accuracy: 0.035\n",
      "Sentence index: 6000\n",
      "Total loss: 0.7465704083442688\n",
      "Accuracy: 0.027\n",
      "Sentence index: 7000\n",
      "Total loss: 0.6836177110671997\n",
      "Accuracy: 0.048\n",
      "Sentence index: 8000\n",
      "Total loss: 0.7154678702354431\n",
      "Accuracy: 0.034\n",
      "Sentence index: 9000\n",
      "Total loss: 0.7998234629631042\n",
      "Accuracy: 0.034\n",
      "Sentence index: 10000\n",
      "Total loss: 0.7788370251655579\n",
      "Accuracy: 0.032\n",
      "Sentence index: 11000\n",
      "Total loss: 0.5717579126358032\n",
      "Accuracy: 0.035\n",
      "Sentence index: 12000\n",
      "Total loss: 0.7386078834533691\n",
      "Accuracy: 0.038\n",
      "Sentence index: 13000\n",
      "Total loss: 0.7186319231987\n",
      "Accuracy: 0.032\n",
      "Sentence index: 14000\n",
      "Total loss: 0.8232313990592957\n",
      "Accuracy: 0.043\n",
      "Sentence index: 15000\n",
      "Total loss: 0.6829845309257507\n",
      "Accuracy: 0.038\n",
      "Sentence index: 16000\n",
      "Total loss: 0.6948740482330322\n",
      "Accuracy: 0.047\n",
      "Sentence index: 17000\n",
      "Total loss: 0.7256754040718079\n",
      "Accuracy: 0.033\n",
      "Sentence index: 18000\n",
      "Total loss: 0.8878743052482605\n",
      "Accuracy: 0.043\n",
      "Sentence index: 19000\n",
      "Total loss: 0.7640625238418579\n",
      "Accuracy: 0.039\n",
      "Sentence index: 20000\n",
      "Total loss: 0.6513198614120483\n",
      "Accuracy: 0.039\n",
      "Sentence index: 21000\n",
      "Total loss: 0.7758247256278992\n",
      "Accuracy: 0.041\n",
      "Sentence index: 22000\n",
      "Total loss: 0.734946072101593\n",
      "Accuracy: 0.037\n",
      "Sentence index: 23000\n",
      "Total loss: 0.7672981023788452\n",
      "Accuracy: 0.031\n",
      "Sentence index: 24000\n",
      "Total loss: 0.7155088186264038\n",
      "Accuracy: 0.046\n",
      "Sentence index: 25000\n",
      "Total loss: 0.7069098353385925\n",
      "Accuracy: 0.029\n",
      "Sentence index: 26000\n",
      "Total loss: 0.6884036660194397\n",
      "Accuracy: 0.03\n",
      "Sentence index: 27000\n",
      "Total loss: 0.7984854578971863\n",
      "Accuracy: 0.033\n",
      "Sentence index: 28000\n",
      "Total loss: 0.7577542066574097\n",
      "Accuracy: 0.031\n",
      "Sentence index: 29000\n",
      "Total loss: 0.6541845202445984\n",
      "Accuracy: 0.042\n",
      "Sentence index: 30000\n",
      "Total loss: 0.651559591293335\n",
      "Accuracy: 0.028\n",
      "Sentence index: 31000\n",
      "Total loss: 0.6866703033447266\n",
      "Accuracy: 0.036\n",
      "Sentence index: 32000\n",
      "Total loss: 0.6310374140739441\n",
      "Accuracy: 0.035\n",
      "Sentence index: 33000\n",
      "Total loss: 0.7355608940124512\n",
      "Accuracy: 0.031\n",
      "Sentence index: 34000\n",
      "Total loss: 0.6944722533226013\n",
      "Accuracy: 0.026\n",
      "Sentence index: 35000\n",
      "Total loss: 0.7236213684082031\n",
      "Accuracy: 0.025\n",
      "Sentence index: 36000\n",
      "Total loss: 0.7079516649246216\n",
      "Accuracy: 0.037\n",
      "Sentence index: 37000\n",
      "Total loss: 0.8299760222434998\n",
      "Accuracy: 0.037\n",
      "Sentence index: 38000\n",
      "Total loss: 0.6976670622825623\n",
      "Accuracy: 0.046\n",
      "Sentence index: 39000\n",
      "Total loss: 0.7377333045005798\n",
      "Accuracy: 0.036\n",
      "Sentence index: 40000\n",
      "Total loss: 0.7715481519699097\n",
      "Accuracy: 0.034\n",
      "Sentence index: 41000\n",
      "Total loss: 0.8328744173049927\n",
      "Accuracy: 0.035\n",
      "Sentence index: 42000\n",
      "Total loss: 0.6409508585929871\n",
      "Accuracy: 0.03\n",
      "Sentence index: 43000\n",
      "Total loss: 0.6151115298271179\n",
      "Accuracy: 0.044\n",
      "Sentence index: 44000\n",
      "Total loss: 0.7550489902496338\n",
      "Accuracy: 0.049\n",
      "Sentence index: 45000\n",
      "Total loss: 0.7754412889480591\n",
      "Accuracy: 0.029\n",
      "Sentence index: 46000\n",
      "Total loss: 0.7107835412025452\n",
      "Accuracy: 0.047\n",
      "Sentence index: 47000\n",
      "Total loss: 0.645200788974762\n",
      "Accuracy: 0.032\n",
      "Sentence index: 48000\n",
      "Total loss: 0.6968549489974976\n",
      "Accuracy: 0.042\n",
      "Sentence index: 49000\n",
      "Total loss: 0.766383171081543\n",
      "Accuracy: 0.046\n",
      "Sentence index: 50000\n",
      "Total loss: 0.7503188252449036\n",
      "Accuracy: 0.042\n",
      "Sentence index: 51000\n",
      "Total loss: 0.8672043681144714\n",
      "Accuracy: 0.031\n",
      "Sentence index: 52000\n",
      "Total loss: 0.5879966020584106\n",
      "Accuracy: 0.035\n",
      "Sentence index: 53000\n",
      "Total loss: 0.7355085611343384\n",
      "Accuracy: 0.04\n",
      "Sentence index: 54000\n",
      "Total loss: 0.7397708296775818\n",
      "Accuracy: 0.051\n",
      "Sentence index: 55000\n",
      "Total loss: 0.6314282417297363\n",
      "Accuracy: 0.033\n",
      "Sentence index: 56000\n",
      "Total loss: 0.7487897872924805\n",
      "Accuracy: 0.039\n",
      "Sentence index: 57000\n",
      "Total loss: 0.7031782269477844\n",
      "Accuracy: 0.045\n",
      "Sentence index: 58000\n",
      "Total loss: 0.7811316847801208\n",
      "Accuracy: 0.036\n",
      "Sentence index: 59000\n",
      "Total loss: 0.7556781768798828\n",
      "Accuracy: 0.04\n",
      "Sentence index: 60000\n",
      "Total loss: 0.8055105805397034\n",
      "Accuracy: 0.04\n",
      "Sentence index: 61000\n",
      "Total loss: 0.7226446866989136\n",
      "Accuracy: 0.051\n",
      "Sentence index: 62000\n",
      "Total loss: 0.725139319896698\n",
      "Accuracy: 0.044\n",
      "Sentence index: 63000\n",
      "Total loss: 0.7003596425056458\n",
      "Accuracy: 0.037\n",
      "Sentence index: 64000\n",
      "Total loss: 0.6874914169311523\n",
      "Accuracy: 0.039\n",
      "Sentence index: 65000\n",
      "Total loss: 0.6661646366119385\n",
      "Accuracy: 0.032\n",
      "Sentence index: 66000\n",
      "Total loss: 0.6371057033538818\n",
      "Accuracy: 0.041\n",
      "Sentence index: 67000\n",
      "Total loss: 0.7402603626251221\n",
      "Accuracy: 0.038\n",
      "Sentence index: 68000\n",
      "Total loss: 0.7097627520561218\n",
      "Accuracy: 0.035\n",
      "Sentence index: 69000\n",
      "Total loss: 0.7964867353439331\n",
      "Accuracy: 0.036\n",
      "Sentence index: 70000\n",
      "Total loss: 0.7379048466682434\n",
      "Accuracy: 0.034\n",
      "Sentence index: 71000\n",
      "Total loss: 0.7021476030349731\n",
      "Accuracy: 0.037\n",
      "Sentence index: 72000\n",
      "Total loss: 0.9293496012687683\n",
      "Accuracy: 0.037\n",
      "Sentence index: 73000\n",
      "Total loss: 0.7754172682762146\n",
      "Accuracy: 0.039\n",
      "Sentence index: 74000\n",
      "Total loss: 0.8216157555580139\n",
      "Accuracy: 0.037\n",
      "Sentence index: 75000\n",
      "Total loss: 0.7358049154281616\n",
      "Accuracy: 0.038\n",
      "Sentence index: 76000\n",
      "Total loss: 0.6177336573600769\n",
      "Accuracy: 0.033\n",
      "Sentence index: 77000\n",
      "Total loss: 0.7120616436004639\n",
      "Accuracy: 0.043\n",
      "Sentence index: 78000\n",
      "Total loss: 0.6778124570846558\n",
      "Accuracy: 0.028\n",
      "Sentence index: 79000\n",
      "Total loss: 0.7584290504455566\n",
      "Accuracy: 0.035\n",
      "Sentence index: 80000\n",
      "Total loss: 0.8183136582374573\n",
      "Accuracy: 0.03\n",
      "Sentence index: 81000\n",
      "Total loss: 0.7043537497520447\n",
      "Accuracy: 0.038\n",
      "Sentence index: 82000\n",
      "Total loss: 0.6966134905815125\n",
      "Accuracy: 0.033\n",
      "Sentence index: 83000\n",
      "Total loss: 0.8669683933258057\n",
      "Accuracy: 0.036\n",
      "Sentence index: 84000\n",
      "Total loss: 0.7274279594421387\n",
      "Accuracy: 0.037\n",
      "Sentence index: 85000\n",
      "Total loss: 0.6640411019325256\n",
      "Accuracy: 0.053\n",
      "Sentence index: 86000\n",
      "Total loss: 0.7385114431381226\n",
      "Accuracy: 0.045\n",
      "Sentence index: 87000\n",
      "Total loss: 0.7483484745025635\n",
      "Accuracy: 0.03\n",
      "Sentence index: 88000\n",
      "Total loss: 0.7151153087615967\n",
      "Accuracy: 0.033\n",
      "Sentence index: 89000\n",
      "Total loss: 0.7194931507110596\n",
      "Accuracy: 0.055\n",
      "Sentence index: 90000\n",
      "Total loss: 0.7537540793418884\n",
      "Accuracy: 0.033\n",
      "Sentence index: 91000\n",
      "Total loss: 0.8221312165260315\n",
      "Accuracy: 0.042\n",
      "Sentence index: 92000\n",
      "Total loss: 0.7329218983650208\n",
      "Accuracy: 0.04\n",
      "Sentence index: 93000\n",
      "Total loss: 0.7810785174369812\n",
      "Accuracy: 0.031\n",
      "Sentence index: 94000\n",
      "Total loss: 0.8442623019218445\n",
      "Accuracy: 0.043\n",
      "Sentence index: 95000\n",
      "Total loss: 0.683323860168457\n",
      "Accuracy: 0.041\n",
      "Sentence index: 96000\n",
      "Total loss: 0.8311719298362732\n",
      "Accuracy: 0.036\n",
      "Sentence index: 97000\n",
      "Total loss: 0.6723830103874207\n",
      "Accuracy: 0.036\n",
      "Sentence index: 98000\n",
      "Total loss: 0.772818386554718\n",
      "Accuracy: 0.037\n",
      "Sentence index: 99000\n",
      "Total loss: 0.7268843054771423\n",
      "Accuracy: 0.038\n",
      "Sentence index: 100000\n",
      "Total loss: 0.6731451749801636\n",
      "Accuracy: 0.038\n",
      "Sentence index: 101000\n",
      "Total loss: 0.7476919889450073\n",
      "Accuracy: 0.04\n",
      "Sentence index: 102000\n",
      "Total loss: 0.6615886688232422\n",
      "Accuracy: 0.027\n",
      "Sentence index: 103000\n",
      "Total loss: 0.7505229711532593\n",
      "Accuracy: 0.027\n",
      "Sentence index: 104000\n",
      "Total loss: 0.7565869092941284\n",
      "Accuracy: 0.031\n",
      "Sentence index: 105000\n",
      "Total loss: 0.7532208561897278\n",
      "Accuracy: 0.045\n",
      "Sentence index: 106000\n",
      "Total loss: 0.9154832363128662\n",
      "Accuracy: 0.033\n",
      "Sentence index: 107000\n",
      "Total loss: 0.8027477860450745\n",
      "Accuracy: 0.039\n",
      "Sentence index: 108000\n",
      "Total loss: 0.7177405953407288\n",
      "Accuracy: 0.038\n",
      "Sentence index: 109000\n",
      "Total loss: 0.5877645611763\n",
      "Accuracy: 0.03\n",
      "Sentence index: 110000\n",
      "Total loss: 0.6333751082420349\n",
      "Accuracy: 0.034\n",
      "Sentence index: 111000\n",
      "Total loss: 0.7002729177474976\n",
      "Accuracy: 0.042\n",
      "Sentence index: 112000\n",
      "Total loss: 0.7106583714485168\n",
      "Accuracy: 0.036\n",
      "Sentence index: 113000\n",
      "Total loss: 0.651387095451355\n",
      "Accuracy: 0.037\n",
      "Sentence index: 114000\n",
      "Total loss: 0.7903822660446167\n",
      "Accuracy: 0.021\n",
      "Sentence index: 115000\n",
      "Total loss: 0.6491567492485046\n",
      "Accuracy: 0.031\n",
      "Sentence index: 116000\n",
      "Total loss: 0.7476139664649963\n",
      "Accuracy: 0.031\n",
      "Sentence index: 117000\n",
      "Total loss: 0.7428257465362549\n",
      "Accuracy: 0.046\n",
      "Sentence index: 118000\n",
      "Total loss: 0.7149315476417542\n",
      "Accuracy: 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence index: 119000\n",
      "Total loss: 0.6968228816986084\n",
      "Accuracy: 0.039\n",
      "Sentence index: 120000\n",
      "Total loss: 0.6684834957122803\n",
      "Accuracy: 0.039\n",
      "Sentence index: 121000\n",
      "Total loss: 0.7644031643867493\n",
      "Accuracy: 0.045\n",
      "Sentence index: 122000\n",
      "Total loss: 0.7840768694877625\n",
      "Accuracy: 0.035\n",
      "Sentence index: 123000\n",
      "Total loss: 0.7109640836715698\n",
      "Accuracy: 0.041\n",
      "Sentence index: 124000\n",
      "Total loss: 0.6807681918144226\n",
      "Accuracy: 0.049\n",
      "Sentence index: 125000\n",
      "Total loss: 0.7106829881668091\n",
      "Accuracy: 0.028\n",
      "Sentence index: 126000\n",
      "Total loss: 0.7672754526138306\n",
      "Accuracy: 0.044\n",
      "Sentence index: 127000\n",
      "Total loss: 0.7463210225105286\n",
      "Accuracy: 0.042\n",
      "Sentence index: 128000\n",
      "Total loss: 0.7222540378570557\n",
      "Accuracy: 0.032\n",
      "Sentence index: 129000\n",
      "Total loss: 0.612622082233429\n",
      "Accuracy: 0.041\n",
      "Sentence index: 130000\n",
      "Total loss: 0.7028218507766724\n",
      "Accuracy: 0.031\n",
      "Sentence index: 131000\n",
      "Total loss: 0.746254026889801\n",
      "Accuracy: 0.032\n",
      "Sentence index: 132000\n",
      "Total loss: 0.7208974957466125\n",
      "Accuracy: 0.036\n",
      "Sentence index: 133000\n",
      "Total loss: 0.7958523631095886\n",
      "Accuracy: 0.034\n",
      "Sentence index: 134000\n",
      "Total loss: 0.7227543592453003\n",
      "Accuracy: 0.026\n",
      "Sentence index: 135000\n",
      "Total loss: 0.6876054406166077\n",
      "Accuracy: 0.043\n",
      "Sentence index: 136000\n",
      "Total loss: 0.650805652141571\n",
      "Accuracy: 0.033\n",
      "Sentence index: 137000\n",
      "Total loss: 0.8643496632575989\n",
      "Accuracy: 0.042\n",
      "Sentence index: 138000\n",
      "Total loss: 0.7533890008926392\n",
      "Accuracy: 0.036\n",
      "Sentence index: 139000\n",
      "Total loss: 0.5767447352409363\n",
      "Accuracy: 0.035\n",
      "Sentence index: 140000\n",
      "Total loss: 0.798064649105072\n",
      "Accuracy: 0.054\n",
      "Sentence index: 141000\n",
      "Total loss: 0.6701185703277588\n",
      "Accuracy: 0.029\n",
      "Sentence index: 142000\n",
      "Total loss: 0.6617932319641113\n",
      "Accuracy: 0.04\n",
      "Sentence index: 143000\n",
      "Total loss: 0.7921801805496216\n",
      "Accuracy: 0.038\n",
      "Sentence index: 144000\n",
      "Total loss: 0.6342282295227051\n",
      "Accuracy: 0.026\n",
      "Sentence index: 145000\n",
      "Total loss: 0.691461980342865\n",
      "Accuracy: 0.035\n",
      "Sentence index: 146000\n",
      "Total loss: 0.7481170892715454\n",
      "Accuracy: 0.043\n",
      "Sentence index: 147000\n",
      "Total loss: 0.6950749158859253\n",
      "Accuracy: 0.035\n",
      "Sentence index: 148000\n",
      "Total loss: 0.7098310589790344\n",
      "Accuracy: 0.043\n",
      "Sentence index: 149000\n",
      "Total loss: 0.6952723860740662\n",
      "Accuracy: 0.037\n",
      "Sentence index: 150000\n",
      "Total loss: 0.825519323348999\n",
      "Accuracy: 0.032\n",
      "Sentence index: 151000\n",
      "Total loss: 0.695314884185791\n",
      "Accuracy: 0.037\n",
      "Sentence index: 152000\n",
      "Total loss: 0.8031845092773438\n",
      "Accuracy: 0.037\n",
      "Sentence index: 153000\n",
      "Total loss: 0.726548969745636\n",
      "Accuracy: 0.032\n",
      "Sentence index: 154000\n",
      "Total loss: 0.7595173120498657\n",
      "Accuracy: 0.026\n",
      "Sentence index: 155000\n",
      "Total loss: 0.6995013952255249\n",
      "Accuracy: 0.027\n",
      "Sentence index: 156000\n",
      "Total loss: 0.667149543762207\n",
      "Accuracy: 0.024\n",
      "Sentence index: 157000\n",
      "Total loss: 0.6838423609733582\n",
      "Accuracy: 0.041\n",
      "Sentence index: 158000\n",
      "Total loss: 0.7895846962928772\n",
      "Accuracy: 0.04\n",
      "Sentence index: 159000\n",
      "Total loss: 0.7081398963928223\n",
      "Accuracy: 0.038\n",
      "Sentence index: 160000\n",
      "Total loss: 0.7013588547706604\n",
      "Accuracy: 0.034\n",
      "Sentence index: 161000\n",
      "Total loss: 0.6468513607978821\n",
      "Accuracy: 0.027\n",
      "Sentence index: 162000\n",
      "Total loss: 0.7089718580245972\n",
      "Accuracy: 0.037\n",
      "Sentence index: 163000\n",
      "Total loss: 1.0341416597366333\n",
      "Accuracy: 0.034\n",
      "Sentence index: 164000\n",
      "Total loss: 0.6857507824897766\n",
      "Accuracy: 0.038\n",
      "Sentence index: 165000\n",
      "Total loss: 0.7745376229286194\n",
      "Accuracy: 0.035\n",
      "Sentence index: 166000\n",
      "Total loss: 0.6748825907707214\n",
      "Accuracy: 0.033\n",
      "Sentence index: 167000\n",
      "Total loss: 0.8029808402061462\n",
      "Accuracy: 0.046\n",
      "Sentence index: 168000\n",
      "Total loss: 0.84864342212677\n",
      "Accuracy: 0.03\n",
      "Sentence index: 169000\n",
      "Total loss: 0.6898614764213562\n",
      "Accuracy: 0.039\n",
      "Sentence index: 170000\n",
      "Total loss: 0.8909475803375244\n",
      "Accuracy: 0.049\n",
      "Sentence index: 171000\n",
      "Total loss: 0.6793995499610901\n",
      "Accuracy: 0.045\n",
      "Sentence index: 172000\n",
      "Total loss: 0.7644808292388916\n",
      "Accuracy: 0.031\n",
      "Sentence index: 173000\n",
      "Total loss: 0.6985201835632324\n",
      "Accuracy: 0.035\n",
      "Sentence index: 174000\n",
      "Total loss: 0.7384788393974304\n",
      "Accuracy: 0.04\n",
      "Sentence index: 175000\n",
      "Total loss: 0.6476383805274963\n",
      "Accuracy: 0.042\n",
      "Sentence index: 176000\n",
      "Total loss: 0.7470290064811707\n",
      "Accuracy: 0.041\n",
      "Sentence index: 177000\n",
      "Total loss: 0.6900525093078613\n",
      "Accuracy: 0.033\n",
      "Sentence index: 178000\n",
      "Total loss: 0.8114373087882996\n",
      "Accuracy: 0.038\n",
      "Sentence index: 179000\n",
      "Total loss: 0.6739323735237122\n",
      "Accuracy: 0.037\n",
      "Sentence index: 180000\n",
      "Total loss: 0.7342158555984497\n",
      "Accuracy: 0.035\n",
      "Sentence index: 181000\n",
      "Total loss: 0.7137120962142944\n",
      "Accuracy: 0.035\n",
      "Sentence index: 182000\n",
      "Total loss: 0.7352469563484192\n",
      "Accuracy: 0.027\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)\n",
    "model = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "\n",
    "train_word_predictor(model, all_sentences, tokenizer, batch_size=10, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./trained_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = WordPredictor(bert_module, len(tokenizer.vocab))\n",
    "model.load_weights(\"./trained_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Qualitatively check predicting result of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [\"My sources have suggested that so far the company sees no reason to change its tax structures , which are perfectly legal .\"]\n",
    "input_ids_vals, input_mask_vals, segment_ids_vals = \\\n",
    "    convert_sentences_to_features(all_sentences, tokenizer)\n",
    "\n",
    "input_masks, labels, pure_ids, segment_ids_vals = \\\n",
    "    make_rand_mask(input_ids_vals, input_mask_vals, len(tokenizer.vocab), segment_ids_vals)\n",
    "\n",
    "bert_inputs = [input_ids_vals, input_masks, segment_ids_vals]\n",
    "masked_ids  = [[i, pure_ids[i]] for i in range(len(pure_ids))]\n",
    "result = model((bert_inputs, masked_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "tf.Tensor([1103], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(input_masks)\n",
    "print(tf.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word: ['the']\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing word: {}\".format(tokenizer.convert_ids_to_tokens([1103])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=\n",
       "array([6150, 1115, 1177, 2732, 1849, 1185, 5302,  117, 1134, 1419],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(result, axis=1, direction=\"DESCENDING\")[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0002001503, shape=(), dtype=float32)\n",
      "['perfectly']\n",
      "tf.Tensor(0.00018320294, shape=(), dtype=float32)\n",
      "['that']\n",
      "tf.Tensor(0.00016705593, shape=(), dtype=float32)\n",
      "['so']\n",
      "tf.Tensor(0.00012558168, shape=(), dtype=float32)\n",
      "['legal']\n",
      "tf.Tensor(0.00010918726, shape=(), dtype=float32)\n",
      "['change']\n",
      "tf.Tensor(0.00010350955, shape=(), dtype=float32)\n",
      "['no']\n",
      "tf.Tensor(9.283192e-05, shape=(), dtype=float32)\n",
      "['sees']\n",
      "tf.Tensor(8.091487e-05, shape=(), dtype=float32)\n",
      "[',']\n",
      "tf.Tensor(7.809748e-05, shape=(), dtype=float32)\n",
      "['which']\n",
      "tf.Tensor(7.575437e-05, shape=(), dtype=float32)\n",
      "['company']\n"
     ]
    }
   ],
   "source": [
    "candidates = [6150, 1115, 1177, 2732, 1849, 1185, 5302,  117, 1134, 1419]\n",
    "for candidate in candidates:\n",
    "    print(result[0, candidate])\n",
    "    print(tokenizer.convert_ids_to_tokens([candidate]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
