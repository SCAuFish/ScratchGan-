{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use BERT to score a blank-filling option\n",
    "### Model based on BERT_Cloze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"Using Tensorflow version: \" + tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "BERT_DIR = \"/home/aufish/Downloads/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_module = hub.KerasLayer(BERT_DIR, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer(vocab_file, do_lower_case=False):\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer(BERT_DIR + \"/assets/vocab.txt\")\n",
    "\n",
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len=50):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=50):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids\n",
    "\n",
    "import random, copy\n",
    "import numpy as np\n",
    "MASK_ID = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "def make_mask(input_ids, input_mask, mask_loc):\n",
    "    ''' \n",
    "    Only make mask for one sentence\n",
    "    input_ids: the ids of words in the sentences\n",
    "    input_mask: initial mask (1 if there is a word; 0 for padding)\n",
    "    mask_loc: which word should be masked, notice that the first in-sentence word has index 1\n",
    "    returns\n",
    "    input_id: the word specified by mask_loc is replaced by [MASK]\n",
    "    input_mask: replace one bit of 1 with 0, meaning that the word will be masked\n",
    "    original_word: the word that is replaced\n",
    "    '''\n",
    "    \n",
    "    new_input_mask = copy.deepcopy(input_mask)\n",
    "    new_input_ids  = copy.deepcopy(input_ids)\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "    original_word = input_ids[mask_loc]\n",
    "    \n",
    "    new_input_ids[mask_loc] = MASK_ID\n",
    "    \n",
    "    assert new_input_mask[mask_loc] == 1\n",
    "    new_input_mask[mask_loc] = 0\n",
    "                \n",
    "    return new_input_ids, new_input_mask, original_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make mask sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_mask, segmend_ids = convert_sentence_to_features(\"I love you\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input_ids, new_input_mask, masked_word = make_mask(input_ids, input_mask, 1)\n",
    "masked_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Blank filler model (from BERT_Cloze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictor(tf.keras.Model):\n",
    "    # The output means, how possible the given word may fit into the blank\n",
    "    def __init__(self, class_num, bert=bert_module, dropout=0.1):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.drop = tf.keras.layers.Dropout(rate=dropout, trainable=True)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            class_num,\n",
    "            activation=None,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name='word_prediction',\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs, mask_loc):\n",
    "        # When passed in, all tensors are stacked in one, split it into a list\n",
    "        # inputs = tf.unstack(tf.cast(inputs, tf.dtypes.int32), axis=1)\n",
    "\n",
    "        pooled, sequential = self.bert(inputs)\n",
    "        \n",
    "        # select one from each batch\n",
    "        s = tf.gather_nd(sequential, [(i, mask_loc[i]) for i in range(sequential.shape[0])])\n",
    "        # s now has shape (batch_size * 768)\n",
    "        \n",
    "        x = self.drop(s)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 model calling sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f702c229eb8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WordPredictor(len(tokenizer.vocab))\n",
    "model.load_weights(\"./word_predictor_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1587 1328 2810 1567 1500 2367 6243 1176 3496 3683], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "mask_loc = 2\n",
    "input_ids, input_mask, segment_ids = convert_sentence_to_features([\"I love you\", \"I love you too\"], tokenizer)\n",
    "new_input_ids, new_input_mask, original_word = make_mask(input_ids, input_mask, mask_loc)\n",
    "\n",
    "output = model([[new_input_ids], [new_input_mask], [segment_ids]], [mask_loc])\n",
    "print(tf.argsort(output, axis=1, direction='DESCENDING')[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell',\n",
       " 'want',\n",
       " 'hope',\n",
       " 'love',\n",
       " 'told',\n",
       " 'ask',\n",
       " 'thank',\n",
       " 'like',\n",
       " 'trust',\n",
       " 'wish']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1587, 1328, 2810, 1567, 1500, 2367, 6243, 1176, 3496, 3683])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 To better design scorer, first investigate what the output is from ScratchGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('/home/aufish/Documents/ScratchGan++')\n",
    "\n",
    "from scratchgan.generators import LSTMGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    'vocab_size' : len(tokenizer.vocab),\n",
    "    'gen_feature_size' : 512,\n",
    "    'num_layers_gen' : [512] * 2,\n",
    "    'max_seq_length' : 50,\n",
    "    'batch_size' : 512,\n",
    "    'layer_norm_gen' : False,\n",
    "    'trainable_embedding_size' : 64,\n",
    "    'gen_input_dropout' : 0.0,\n",
    "    'gen_output_dropout' : 0.0,\n",
    "    'pad_int' : 0,\n",
    "    'embedding_source' : ,\n",
    "    'vocab_file' : \n",
    "}\n",
    "\n",
    "\n",
    "  gen = generators.LSTMGen(\n",
    "      vocab_size=vocab_size,\n",
    "      feature_sizes=[config.gen_feature_size] * config.num_layers_gen,\n",
    "      max_sequence_length=reader.MAX_TOKENS_SEQUENCE[config.dataset],\n",
    "      batch_size=config.batch_size,\n",
    "      use_layer_norm=config.layer_norm_gen,\n",
    "      trainable_embedding_size=config.trainable_embedding_size,\n",
    "      input_dropout=config.gen_input_dropout,\n",
    "      output_dropout=config.gen_output_dropout,\n",
    "      pad_token=reader.PAD_INT,\n",
    "      embedding_source=embedding_source,\n",
    "      vocab_file=vocab_file,\n",
    "  )\n",
    "\n",
    "gen = generators.LSTMGen(\n",
    "      vocab_size=vocab_size,\n",
    "      feature_sizes=[config.gen_feature_size] * config.num_layers_gen,\n",
    "      max_sequence_length=reader.MAX_TOKENS_SEQUENCE[config.dataset],\n",
    "      batch_size=config.batch_size,\n",
    "      use_layer_norm=config.layer_norm_gen,\n",
    "      trainable_embedding_size=config.trainable_embedding_size,\n",
    "      input_dropout=config.gen_input_dropout,\n",
    "      output_dropout=config.gen_output_dropout,\n",
    "      pad_token=reader.PAD_INT,\n",
    "      embedding_source=embedding_source,\n",
    "      vocab_file=vocab_file,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(model, sentences):\n",
    "    # Given a list sentence, use WordPredictor to estimate how good each word is\n",
    "    # returned result should be a tensor with dimension sentence_num x seq_length\n",
    "    input_ids, input_mask, segment_ids = convert_sentences_to_features(sentences, tokenizer)\n",
    "    for i in range(len(input_ids) - 2):\n",
    "        # minus 2 because starting and ending tokens do not need to be masked\n",
    "        new_input_ids, new_input_mask, original_word = make_mask(input_ids, input_mask, i)\n",
    "        \n",
    "        output = model([[new_input_ids], [new_input_mask]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
